{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e845b0ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting Authentication ===\n",
      "Found existing token file, loading credentials...\n",
      "Credentials expired, refreshing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-22 12:41:12,564 - INFO - file_cache is only supported with oauth2client<4.0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Gmail service...\n",
      "Gmail service ready!\n",
      "\n",
      "=== Fetching 25 most recent emails ===\n",
      "Found 8 emails in inbox\n",
      "\n",
      "=== Processing Emails ===\n",
      "\n",
      "Processing Email #1 (ID: 1982174cf77fb3a3)\n",
      "Title: Hybrid/Local Analytics Engineer (MS Fabric/12+)\n",
      "Job_ID: GA-768025\n",
      "Due_date: 07/16\n",
      "\n",
      "Processing Email #2 (ID: 198217128c79c421)\n",
      "Title: Hybrid/Local Privacy Officer/Security Analyst (CISA/CIPM/CIPP)\n",
      "Job_ID: NC-768095\n",
      "Due_date: 07/24\n",
      "\n",
      "Processing Email #3 (ID: 198216d126f86664)\n",
      "Title: Remote Sitefinity Developer (12+)\n",
      "Job_ID: NC-765767\n",
      "Due_date: 06/25\n",
      "\n",
      "Processing Email #4 (ID: 198216c4da407075)\n",
      "Title: Remote Sitefinity Developer\n",
      "Job_ID: NC-765767\n",
      "Due_date: 07/21\n",
      "\n",
      "Processing Email #5 (ID: 1982169418497b56)\n",
      "Title: Remote/Local GIS Analyst (12+)\n",
      "Job_ID: NC-767681\n",
      "Due_date: 07/18\n",
      "\n",
      "Processing Email #6 (ID: 198216585c548259)\n",
      "Title: Hybrid/Local (Lansing ONLY) .NET Developer (12+)\n",
      "Job_ID: MI-144480\n",
      "Due_date: 07/21\n",
      "\n",
      "Processing Email #7 (ID: 1982135e4ab36a6a)\n",
      "Title: Hybrid/Local (Lansing 90 min) Full-stack Java Developer (15+)\n",
      "Job_ID: MI-144516\n",
      "Due_date: 07/22\n",
      "\n",
      "Processing Email #8 (ID: 1982128df901294a)\n",
      "Title: Hybrid/Local VA Govt BA\n",
      "Job_ID: VA-767445\n",
      "Due_date: 07/23\n",
      "\n",
      "=== Final Results ===\n",
      "        Email ID                                                          Title    Job_ID Due_date\n",
      "1982174cf77fb3a3                Hybrid/Local Analytics Engineer (MS Fabric/12+) GA-768025    07/16\n",
      "198217128c79c421 Hybrid/Local Privacy Officer/Security Analyst (CISA/CIPM/CIPP) NC-768095    07/24\n",
      "198216d126f86664                              Remote Sitefinity Developer (12+) NC-765767    06/25\n",
      "198216c4da407075                                    Remote Sitefinity Developer NC-765767    07/21\n",
      "1982169418497b56                                 Remote/Local GIS Analyst (12+) NC-767681    07/18\n",
      "198216585c548259               Hybrid/Local (Lansing ONLY) .NET Developer (12+) MI-144480    07/21\n",
      "1982135e4ab36a6a  Hybrid/Local (Lansing 90 min) Full-stack Java Developer (15+) MI-144516    07/22\n",
      "1982128df901294a                                        Hybrid/Local VA Govt BA VA-767445    07/23\n",
      "An error occurred: [Errno 13] Permission denied: 'extracted_job_details.csv'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "import re\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from google.oauth2.credentials import Credentials\n",
    "from google.auth.transport.requests import Request\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from googleapiclient.discovery import build\n",
    "from base64 import urlsafe_b64decode\n",
    "import pytz\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "def auto_authenticate_google():\n",
    "    \"\"\"Authenticates with Gmail API using OAuth 2.0\"\"\"\n",
    "    print(\"\\n=== Starting Authentication ===\")\n",
    "    creds = None\n",
    "    token_path = 'token.json'\n",
    "    SCOPES = ['https://www.googleapis.com/auth/gmail.readonly']\n",
    "    \n",
    "    if os.path.exists(token_path):\n",
    "        print(\"Found existing token file, loading credentials...\")\n",
    "        try:\n",
    "            creds = Credentials.from_authorized_user_file(token_path, SCOPES)\n",
    "            if creds.expired and creds.refresh_token:\n",
    "                print(\"Credentials expired, refreshing...\")\n",
    "                creds.refresh(Request())\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading credentials: {e}\")\n",
    "            creds = None\n",
    "    \n",
    "    if not creds or not creds.valid:\n",
    "        print(\"No valid credentials found, initiating OAuth flow...\")\n",
    "        try:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file('client.json', SCOPES)\n",
    "            creds = flow.run_local_server(port=8080)\n",
    "            token_data = json.loads(creds.to_json())\n",
    "            token_data['creation_time'] = datetime.now(pytz.UTC).isoformat()\n",
    "            with open(token_path, 'w') as token:\n",
    "                json.dump(token_data, token)\n",
    "            print(\"Authentication successful! Token saved.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Authentication failed: {e}\")\n",
    "            raise\n",
    "    \n",
    "    try:\n",
    "        print(\"Building Gmail service...\")\n",
    "        gmail_service = build('gmail', 'v1', credentials=creds)\n",
    "        print(\"Gmail service ready!\")\n",
    "        return gmail_service\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to build Gmail service: {e}\")\n",
    "        raise\n",
    "\n",
    "def get_recent_emails(service, max_results=30):\n",
    "    \"\"\"Fetches the most recent emails from the inbox\"\"\"\n",
    "    print(f\"\\n=== Fetching {max_results} most recent emails ===\")\n",
    "    try:\n",
    "        results = service.users().messages().list(\n",
    "            userId=\"me\",\n",
    "            labelIds=['INBOX'],\n",
    "            maxResults=max_results\n",
    "        ).execute()\n",
    "        messages = results.get('messages', [])\n",
    "        print(f\"Found {len(messages)} emails in inbox\")\n",
    "        return messages\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching emails: {e}\")\n",
    "        return []\n",
    "\n",
    "def decode_base64(data):\n",
    "    \"\"\"Decodes base64 email content with proper padding\"\"\"\n",
    "    missing_padding = len(data) % 4\n",
    "    if missing_padding:\n",
    "        data += '=' * (4 - missing_padding)\n",
    "    return urlsafe_b64decode(data)\n",
    "\n",
    "def extract_email_body(service, message_id):\n",
    "    \"\"\"Extracts and decodes the email body content\"\"\"\n",
    "    try:\n",
    "        msg = service.users().messages().get(\n",
    "            userId=\"me\",\n",
    "            id=message_id,\n",
    "            format='full'\n",
    "        ).execute()\n",
    "        \n",
    "        payload = msg.get('payload', {})\n",
    "        \n",
    "        # Extract body from payload\n",
    "        body = \"\"\n",
    "        if 'body' in payload and 'data' in payload['body']:\n",
    "            body = decode_base64(payload['body']['data']).decode('utf-8', errors='ignore')\n",
    "        elif 'parts' in payload:\n",
    "            for part in payload['parts']:\n",
    "                if part.get('mimeType') in ['text/plain', 'text/html']:\n",
    "                    if 'body' in part and 'data' in part['body']:\n",
    "                        body = decode_base64(part['body']['data']).decode('utf-8', errors='ignore')\n",
    "                        break\n",
    "                elif 'parts' in part:\n",
    "                    for subpart in part['parts']:\n",
    "                        if subpart.get('mimeType') in ['text/plain', 'text/html']:\n",
    "                            if 'body' in subpart and 'data' in subpart['body']:\n",
    "                                body = decode_base64(subpart['body']['data']).decode('utf-8', errors='ignore')\n",
    "                                break\n",
    "        \n",
    "        return body if body else None\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing email {message_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_job_details(body):\n",
    "    \"\"\"Enhanced version focusing on extracting dates from parentheses after Job IDs\"\"\"\n",
    "    job_data = {\n",
    "        'Title': None,\n",
    "        'Job_ID': None,\n",
    "        'Due_date': None\n",
    "    }\n",
    "    \n",
    "    if not body:\n",
    "        return job_data\n",
    "    \n",
    "    # Updated Title extraction pattern to catch:\n",
    "    # 1. Hybrid/Local, Remote/Local, Onsite/Local (original pattern)\n",
    "    # 2. Standalone Remote, Hybrid, Onsite (new addition)\n",
    "    # 3. Followed by optional text until next pattern or end\n",
    "    title_pattern = r'(?i)((?:Hybrid|Onsite|Remote)(?:\\s*/\\s*Local)?[^(]*\\(.*?\\)|(?:Hybrid|Onsite|Remote)(?:\\s*/\\s*Local)?.*?)(?=\\s+with\\s+|\\s*\\(|$|\\n)'\n",
    "    title_match = re.search(title_pattern, body)\n",
    "    if title_match:\n",
    "        job_data['Title'] = title_match.group(1).strip()\n",
    "    \n",
    "    # Rest of the function remains the same...\n",
    "    # Job ID extraction (unchanged)\n",
    "    id_pattern = r'(?i)(?:job\\s*|req\\s*|position\\s*)?(?:id\\s*|#\\s*|num\\s*|number\\s*)?[:#]?\\s*([A-Za-z]{2,}-?\\d{3,})'\n",
    "    id_match = re.search(id_pattern, body)\n",
    "    if id_match:\n",
    "        job_id = id_match.group(1).strip()\n",
    "        job_data['Job_ID'] = job_id\n",
    "        \n",
    "        # === NEW: Focused date extraction from parentheses after Job ID ===\n",
    "        # Looks for pattern: Job ID followed by parentheses with digits\n",
    "        date_match = re.search(\n",
    "            r'{}\\s*\\((\\d+)\\)'.format(re.escape(job_id)),\n",
    "            body\n",
    "        )\n",
    "        if date_match:\n",
    "            # Extract the last 4 digits and format as MM/DD\n",
    "            full_number = date_match.group(1)\n",
    "            last_four = full_number[-4:]  # Always take last 4 digits\n",
    "            if len(last_four) == 4:  # Ensure we have exactly 4 digits\n",
    "                job_data['Due_date'] = f\"{last_four[:2]}/{last_four[2:]}\"\n",
    "    \n",
    "    # Fallback: If no Job ID found, try standalone ID pattern (unchanged)\n",
    "    else:\n",
    "        standalone_id = r'(?<!\\w)([A-Za-z]{2,}-?\\d{3,})(?!\\w)'\n",
    "        id_match = re.search(standalone_id, body)\n",
    "        if id_match:\n",
    "            job_data['Job_ID'] = id_match.group(1).strip()\n",
    "    \n",
    "    # Fallback: Check for standalone parentheses dates if not found after Job ID\n",
    "    if not job_data['Due_date']:\n",
    "        standalone_date_match = re.search(r'\\((\\d+)\\)', body)\n",
    "        if standalone_date_match:\n",
    "            full_number = standalone_date_match.group(1)\n",
    "            last_four = full_number[-4:]  # Take last 4 digits\n",
    "            if len(last_four) == 4:\n",
    "                job_data['Due_date'] = f\"{last_four[:2]}/{last_four[2:]}\"\n",
    "    \n",
    "    return job_data\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to process emails and extract job details\"\"\"\n",
    "    try:\n",
    "        # Initialize results table\n",
    "        results = []\n",
    "        \n",
    "        # Step 1: Authenticate\n",
    "        service = auto_authenticate_google()\n",
    "        \n",
    "        # Step 2: Get recent emails\n",
    "        messages = get_recent_emails(service, max_results=25)\n",
    "        \n",
    "        if not messages:\n",
    "            print(\"No emails found in inbox\")\n",
    "            return\n",
    "        \n",
    "        # Step 3: Process each email\n",
    "        print(\"\\n=== Processing Emails ===\")\n",
    "        for i, message in enumerate(messages, 1):\n",
    "            message_id = message['id']\n",
    "            print(f\"\\nProcessing Email #{i} (ID: {message_id})\")\n",
    "            \n",
    "            # Step 4: Extract body\n",
    "            body = extract_email_body(service, message_id)\n",
    "            \n",
    "            if body:\n",
    "                # Step 5: Extract job details\n",
    "                job_details = extract_job_details(body)\n",
    "                job_details['Email ID'] = message_id\n",
    "                results.append(job_details)\n",
    "                \n",
    "                # Print found details\n",
    "                print(f\"Title: {job_details['Title']}\")\n",
    "                print(f\"Job_ID: {job_details['Job_ID']}\")\n",
    "                print(f\"Due_date: {job_details['Due_date']}\")\n",
    "            else:\n",
    "                print(\"No body content could be extracted\")\n",
    "        \n",
    "        # Create and display results table\n",
    "        df = pd.DataFrame(results)\n",
    "        if not df.empty:\n",
    "            print(\"\\n=== Final Results ===\")\n",
    "            print(df[['Email ID', 'Title', 'Job_ID', 'Due_date']].to_string(index=False))\n",
    "            \n",
    "            # Save to CSV\n",
    "            df.to_csv('extracted_job_details.csv', index=False)\n",
    "            print(\"\\nResults saved to 'extracted_job_details.csv'\")\n",
    "        else:\n",
    "            print(\"\\nNo job details found in any emails\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "375800b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-22 11:39:02,438 - INFO - file_cache is only supported with oauth2client<4.0.0\n",
      "2025-07-22 11:39:02,454 - INFO - Searching for emails with job ID: GA-768025\n",
      "2025-07-22 11:39:03,677 - INFO - Found 3 emails for GA-768025\n",
      "2025-07-22 11:39:04,178 - INFO - Searching for emails with job ID: NC-768095\n",
      "2025-07-22 11:39:04,639 - INFO - Found 7 emails for NC-768095\n",
      "2025-07-22 11:39:05,154 - INFO - Searching for emails with job ID: NC-765767\n",
      "2025-07-22 11:39:05,622 - INFO - Found 6 emails for NC-765767\n",
      "2025-07-22 11:39:06,128 - INFO - Searching for emails with job ID: NC-765767\n",
      "2025-07-22 11:39:06,589 - INFO - Found 6 emails for NC-765767\n",
      "2025-07-22 11:39:07,093 - INFO - Searching for emails with job ID: NC-767681\n",
      "2025-07-22 11:39:08,708 - INFO - Found 6 emails for NC-767681\n",
      "2025-07-22 11:39:09,223 - INFO - Searching for emails with job ID: MI-144480\n",
      "2025-07-22 11:39:11,146 - INFO - Found 11 emails for MI-144480\n",
      "2025-07-22 11:39:11,658 - INFO - Searching for emails with job ID: MI-144516\n",
      "2025-07-22 11:39:12,460 - INFO - Found 1 emails for MI-144516\n",
      "2025-07-22 11:39:12,974 - INFO - Searching for emails with job ID: VA-767445\n",
      "2025-07-22 11:39:13,456 - INFO - Found 0 emails for VA-767445\n",
      "2025-07-22 11:39:13,964 - INFO - Successfully updated CSV file: extracted_job_details.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "from google.oauth2.credentials import Credentials\n",
    "from google.auth.transport.requests import Request\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from googleapiclient.discovery import build\n",
    "from base64 import urlsafe_b64decode\n",
    "import pytz\n",
    "import time\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "def auto_authenticate_google():\n",
    "    \"\"\"Automatically authenticates with Google APIs.\"\"\"\n",
    "    creds = None\n",
    "    token_path = 'token1.json'\n",
    "    SCOPES = ['https://www.googleapis.com/auth/gmail.readonly']\n",
    "    \n",
    "    if os.path.exists(token_path):\n",
    "        try:\n",
    "            creds = Credentials.from_authorized_user_file(token_path, SCOPES)\n",
    "            if creds.expired and creds.refresh_token:\n",
    "                creds.refresh(Request())\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading credentials: {e}\")\n",
    "            creds = None\n",
    "    \n",
    "    if not creds or not creds.valid:\n",
    "        try:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file('client1.json', SCOPES)\n",
    "            creds = flow.run_local_server(port=8080)\n",
    "            token_data = json.loads(creds.to_json())\n",
    "            token_data['creation_time'] = datetime.now(pytz.UTC).isoformat()\n",
    "            with open(token_path, 'w') as token:\n",
    "                json.dump(token_data, token)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Authentication failed: {e}\")\n",
    "            raise\n",
    "    \n",
    "    try:\n",
    "        gmail_service = build('gmail', 'v1', credentials=creds)\n",
    "        return gmail_service\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to build services: {e}\")\n",
    "        raise\n",
    "\n",
    "def count_emails_for_job_id(service, job_id):\n",
    "    \"\"\"Count emails containing the job ID in the inbox.\"\"\"\n",
    "    try:\n",
    "        query = f'\"{job_id}\"'  # Using quotes for exact matching\n",
    "        results = service.users().messages().list(\n",
    "            userId=\"me\",\n",
    "            q=query,\n",
    "            labelIds=['INBOX']\n",
    "        ).execute()\n",
    "        return results.get('resultSizeEstimate', 0)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error counting emails for {job_id}: {e}\")\n",
    "        return 0\n",
    "\n",
    "def process_job_ids(csv_path, service):\n",
    "    \"\"\"Process job IDs from CSV and update with email counts.\"\"\"\n",
    "    # Read the existing CSV file\n",
    "    try:\n",
    "        with open(csv_path, 'r') as file:\n",
    "            reader = csv.DictReader(file)\n",
    "            rows = list(reader)\n",
    "            fieldnames = reader.fieldnames\n",
    "            \n",
    "        # Add 'No_of_emails' column if it doesn't exist\n",
    "        if 'No_of_emails' not in fieldnames:\n",
    "            fieldnames.append('No_of_emails')\n",
    "            for row in rows:\n",
    "                row['No_of_emails'] = '0'\n",
    "    \n",
    "        # Process each job ID\n",
    "        for row in rows:\n",
    "            job_id = row.get('Job ID', '').strip() or row.get('Job_ID', '').strip()\n",
    "            if job_id:  # Only process if job_id exists\n",
    "                logging.info(f\"Searching for emails with job ID: {job_id}\")\n",
    "                email_count = count_emails_for_job_id(service, job_id)\n",
    "                row['No_of_emails'] = str(email_count)\n",
    "                logging.info(f\"Found {email_count} emails for {job_id}\")\n",
    "                time.sleep(0.5)  # Add small delay to avoid rate limiting\n",
    "    \n",
    "        # Write back to the CSV file\n",
    "        with open(csv_path, 'w', newline='') as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(rows)\n",
    "    \n",
    "        logging.info(f\"Successfully updated CSV file: {csv_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing CSV file: {e}\")\n",
    "        raise\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Authenticate with Gmail API\n",
    "        service = auto_authenticate_google()\n",
    "        \n",
    "        # Path to your existing CSV file\n",
    "        csv_path = 'extracted_job_details.csv'\n",
    "        \n",
    "        # Process the job IDs and update the CSV\n",
    "        process_job_ids(csv_path, service)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in main execution: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75745da4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06aa53f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0226dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2287c36d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e657a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Authenticating Primary Gmail Account ===\n",
      "Found primary token file, loading credentials...\n",
      "Primary credentials expired, refreshing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-22 15:36:17,479 - INFO - file_cache is only supported with oauth2client<4.0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building primary Gmail service...\n",
      "Primary Gmail service ready!\n",
      "\n",
      "=== Fetching 25 most recent emails ===\n",
      "Found 8 emails in inbox\n",
      "\n",
      "=== Processing Emails from Primary Account ===\n",
      "\n",
      "Processing Email #1 (ID: 1982174cf77fb3a3)\n",
      "Error processing email 1982174cf77fb3a3: The read operation timed out\n",
      "No body content could be extracted\n",
      "\n",
      "Processing Email #2 (ID: 198217128c79c421)\n",
      "Title: Hybrid/Local Privacy Officer/Security Analyst (CISA/CIPM/CIPP)\n",
      "Job_ID: NC-768095\n",
      "Due_date: 07/24\n",
      "\n",
      "Processing Email #3 (ID: 198216d126f86664)\n",
      "Title: Remote Sitefinity Developer (12+)\n",
      "Job_ID: NC-765767\n",
      "Due_date: 06/25\n",
      "\n",
      "Processing Email #4 (ID: 198216c4da407075)\n",
      "Title: Remote Sitefinity Developer\n",
      "Job_ID: NC-765767\n",
      "Due_date: 07/21\n",
      "\n",
      "Processing Email #5 (ID: 1982169418497b56)\n",
      "Title: Remote/Local GIS Analyst (12+)\n",
      "Job_ID: NC-767681\n",
      "Due_date: 07/18\n",
      "\n",
      "Processing Email #6 (ID: 198216585c548259)\n",
      "Title: Hybrid/Local (Lansing ONLY) .NET Developer (12+)\n",
      "Job_ID: MI-144480\n",
      "Due_date: 07/21\n",
      "\n",
      "Processing Email #7 (ID: 1982135e4ab36a6a)\n",
      "Title: Hybrid/Local (Lansing 90 min) Full-stack Java Developer (15+)\n",
      "Job_ID: MI-144516\n",
      "Due_date: 07/22\n",
      "\n",
      "Processing Email #8 (ID: 1982128df901294a)\n",
      "Title: Hybrid/Local VA Govt BA\n",
      "Job_ID: VA-767445\n",
      "Due_date: 07/23\n",
      "\n",
      "=== Final Results ===\n",
      "        Email ID                                                          Title    Job_ID Due_date\n",
      "198217128c79c421 Hybrid/Local Privacy Officer/Security Analyst (CISA/CIPM/CIPP) NC-768095    07/24\n",
      "198216d126f86664                              Remote Sitefinity Developer (12+) NC-765767    06/25\n",
      "198216c4da407075                                    Remote Sitefinity Developer NC-765767    07/21\n",
      "1982169418497b56                                 Remote/Local GIS Analyst (12+) NC-767681    07/18\n",
      "198216585c548259               Hybrid/Local (Lansing ONLY) .NET Developer (12+) MI-144480    07/21\n",
      "1982135e4ab36a6a  Hybrid/Local (Lansing 90 min) Full-stack Java Developer (15+) MI-144516    07/22\n",
      "1982128df901294a                                        Hybrid/Local VA Govt BA VA-767445    07/23\n",
      "\n",
      "Results saved to 'extracted_job_details.csv'\n",
      "\n",
      "=== Authenticating Secondary Gmail Account ===\n",
      "Found secondary token file, loading credentials...\n",
      "Secondary credentials expired, refreshing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-22 15:37:37,743 - INFO - file_cache is only supported with oauth2client<4.0.0\n",
      "2025-07-22 15:37:37,766 - INFO - Searching for emails with job ID: NC-768095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building secondary Gmail service...\n",
      "Secondary Gmail service ready!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-22 15:37:38,898 - INFO - Found 7 emails for NC-768095\n",
      "2025-07-22 15:37:39,419 - INFO - Searching for emails with job ID: NC-765767\n",
      "2025-07-22 15:37:39,909 - INFO - Found 6 emails for NC-765767\n",
      "2025-07-22 15:37:40,416 - INFO - Searching for emails with job ID: NC-765767\n",
      "2025-07-22 15:37:40,889 - INFO - Found 6 emails for NC-765767\n",
      "2025-07-22 15:37:41,397 - INFO - Searching for emails with job ID: NC-767681\n",
      "2025-07-22 15:37:42,155 - INFO - Found 6 emails for NC-767681\n",
      "2025-07-22 15:37:42,664 - INFO - Searching for emails with job ID: MI-144480\n",
      "2025-07-22 15:37:43,130 - INFO - Found 11 emails for MI-144480\n",
      "2025-07-22 15:37:43,638 - INFO - Searching for emails with job ID: MI-144516\n",
      "2025-07-22 15:37:44,365 - INFO - Found 1 emails for MI-144516\n",
      "2025-07-22 15:37:44,880 - INFO - Searching for emails with job ID: VA-767445\n",
      "2025-07-22 15:38:19,416 - INFO - Found 0 emails for VA-767445\n",
      "2025-07-22 15:38:19,944 - INFO - Successfully updated CSV file: extracted_job_details.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Added email counts from secondary account to the CSV file\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "import re\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from google.oauth2.credentials import Credentials\n",
    "from google.auth.transport.requests import Request\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from googleapiclient.discovery import build\n",
    "from base64 import urlsafe_b64decode\n",
    "import pytz\n",
    "import csv\n",
    "import time\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "def auto_authenticate_primary_gmail():\n",
    "    \"\"\"Authenticates with primary Gmail account (token.json)\"\"\"\n",
    "    print(\"\\n=== Authenticating Primary Gmail Account ===\")\n",
    "    creds = None\n",
    "    token_path = 'token.json'\n",
    "    SCOPES = ['https://www.googleapis.com/auth/gmail.readonly']\n",
    "    \n",
    "    if os.path.exists(token_path):\n",
    "        print(\"Found primary token file, loading credentials...\")\n",
    "        try:\n",
    "            creds = Credentials.from_authorized_user_file(token_path, SCOPES)\n",
    "            if creds.expired and creds.refresh_token:\n",
    "                print(\"Primary credentials expired, refreshing...\")\n",
    "                creds.refresh(Request())\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading primary credentials: {e}\")\n",
    "            creds = None\n",
    "    \n",
    "    if not creds or not creds.valid:\n",
    "        print(\"No valid primary credentials found, initiating OAuth flow...\")\n",
    "        try:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file('client.json', SCOPES)\n",
    "            creds = flow.run_local_server(port=8080)\n",
    "            token_data = json.loads(creds.to_json())\n",
    "            token_data['creation_time'] = datetime.now(pytz.UTC).isoformat()\n",
    "            with open(token_path, 'w') as token:\n",
    "                json.dump(token_data, token)\n",
    "            print(\"Primary authentication successful! Token saved.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Primary authentication failed: {e}\")\n",
    "            raise\n",
    "    \n",
    "    try:\n",
    "        print(\"Building primary Gmail service...\")\n",
    "        gmail_service = build('gmail', 'v1', credentials=creds)\n",
    "        print(\"Primary Gmail service ready!\")\n",
    "        return gmail_service\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to build primary Gmail service: {e}\")\n",
    "        raise\n",
    "\n",
    "def auto_authenticate_secondary_gmail():\n",
    "    \"\"\"Authenticates with secondary Gmail account (token1.json)\"\"\"\n",
    "    print(\"\\n=== Authenticating Secondary Gmail Account ===\")\n",
    "    creds = None\n",
    "    token_path = 'token1.json'\n",
    "    SCOPES = ['https://www.googleapis.com/auth/gmail.readonly']\n",
    "    \n",
    "    if os.path.exists(token_path):\n",
    "        print(\"Found secondary token file, loading credentials...\")\n",
    "        try:\n",
    "            creds = Credentials.from_authorized_user_file(token_path, SCOPES)\n",
    "            if creds.expired and creds.refresh_token:\n",
    "                print(\"Secondary credentials expired, refreshing...\")\n",
    "                creds.refresh(Request())\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading secondary credentials: {e}\")\n",
    "            creds = None\n",
    "    \n",
    "    if not creds or not creds.valid:\n",
    "        print(\"No valid secondary credentials found, initiating OAuth flow...\")\n",
    "        try:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file('client1.json', SCOPES)\n",
    "            creds = flow.run_local_server(port=8081)  # Different port than primary\n",
    "            token_data = json.loads(creds.to_json())\n",
    "            token_data['creation_time'] = datetime.now(pytz.UTC).isoformat()\n",
    "            with open(token_path, 'w') as token:\n",
    "                json.dump(token_data, token)\n",
    "            print(\"Secondary authentication successful! Token saved.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Secondary authentication failed: {e}\")\n",
    "            raise\n",
    "    \n",
    "    try:\n",
    "        print(\"Building secondary Gmail service...\")\n",
    "        gmail_service = build('gmail', 'v1', credentials=creds)\n",
    "        print(\"Secondary Gmail service ready!\")\n",
    "        return gmail_service\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to build secondary Gmail service: {e}\")\n",
    "        raise\n",
    "\n",
    "def get_recent_emails(service, max_results=30):\n",
    "    \"\"\"Fetches the most recent emails from the inbox\"\"\"\n",
    "    print(f\"\\n=== Fetching {max_results} most recent emails ===\")\n",
    "    try:\n",
    "        results = service.users().messages().list(\n",
    "            userId=\"me\",\n",
    "            labelIds=['INBOX'],\n",
    "            maxResults=max_results\n",
    "        ).execute()\n",
    "        messages = results.get('messages', [])\n",
    "        print(f\"Found {len(messages)} emails in inbox\")\n",
    "        return messages\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching emails: {e}\")\n",
    "        return []\n",
    "\n",
    "def decode_base64(data):\n",
    "    \"\"\"Decodes base64 email content with proper padding\"\"\"\n",
    "    missing_padding = len(data) % 4\n",
    "    if missing_padding:\n",
    "        data += '=' * (4 - missing_padding)\n",
    "    return urlsafe_b64decode(data)\n",
    "\n",
    "def extract_email_body(service, message_id):\n",
    "    \"\"\"Extracts and decodes the email body content\"\"\"\n",
    "    try:\n",
    "        msg = service.users().messages().get(\n",
    "            userId=\"me\",\n",
    "            id=message_id,\n",
    "            format='full'\n",
    "        ).execute()\n",
    "        \n",
    "        payload = msg.get('payload', {})\n",
    "        \n",
    "        # Extract body from payload\n",
    "        body = \"\"\n",
    "        if 'body' in payload and 'data' in payload['body']:\n",
    "            body = decode_base64(payload['body']['data']).decode('utf-8', errors='ignore')\n",
    "        elif 'parts' in payload:\n",
    "            for part in payload['parts']:\n",
    "                if part.get('mimeType') in ['text/plain', 'text/html']:\n",
    "                    if 'body' in part and 'data' in part['body']:\n",
    "                        body = decode_base64(part['body']['data']).decode('utf-8', errors='ignore')\n",
    "                        break\n",
    "                elif 'parts' in part:\n",
    "                    for subpart in part['parts']:\n",
    "                        if subpart.get('mimeType') in ['text/plain', 'text/html']:\n",
    "                            if 'body' in subpart and 'data' in subpart['body']:\n",
    "                                body = decode_base64(subpart['body']['data']).decode('utf-8', errors='ignore')\n",
    "                                break\n",
    "        \n",
    "        return body if body else None\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing email {message_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_job_details(body):\n",
    "    \"\"\"Enhanced version focusing on extracting dates from parentheses after Job IDs\"\"\"\n",
    "    job_data = {\n",
    "        'Title': None,\n",
    "        'Job_ID': None,\n",
    "        'Due_date': None\n",
    "    }\n",
    "    \n",
    "    if not body:\n",
    "        return job_data\n",
    "    \n",
    "    # Title extraction pattern\n",
    "    title_pattern = r'(?i)((?:Hybrid|Onsite|Remote)(?:\\s*/\\s*Local)?[^(]*\\(.*?\\)|(?:Hybrid|Onsite|Remote)(?:\\s*/\\s*Local)?.*?)(?=\\s+with\\s+|\\s*\\(|$|\\n)'\n",
    "    title_match = re.search(title_pattern, body)\n",
    "    if title_match:\n",
    "        job_data['Title'] = title_match.group(1).strip()\n",
    "    \n",
    "    # Job ID extraction\n",
    "    id_pattern = r'(?i)(?:job\\s*|req\\s*|position\\s*)?(?:id\\s*|#\\s*|num\\s*|number\\s*)?[:#]?\\s*([A-Za-z]{2,}-?\\d{3,})'\n",
    "    id_match = re.search(id_pattern, body)\n",
    "    if id_match:\n",
    "        job_id = id_match.group(1).strip()\n",
    "        job_data['Job_ID'] = job_id\n",
    "        \n",
    "        # Date extraction from parentheses after Job ID\n",
    "        date_match = re.search(\n",
    "            r'{}\\s*\\((\\d+)\\)'.format(re.escape(job_id)),\n",
    "            body\n",
    "        )\n",
    "        if date_match:\n",
    "            full_number = date_match.group(1)\n",
    "            last_four = full_number[-4:]\n",
    "            if len(last_four) == 4:\n",
    "                job_data['Due_date'] = f\"{last_four[:2]}/{last_four[2:]}\"\n",
    "    \n",
    "    # Fallback: If no Job ID found, try standalone ID pattern\n",
    "    else:\n",
    "        standalone_id = r'(?<!\\w)([A-Za-z]{2,}-?\\d{3,})(?!\\w)'\n",
    "        id_match = re.search(standalone_id, body)\n",
    "        if id_match:\n",
    "            job_data['Job_ID'] = id_match.group(1).strip()\n",
    "    \n",
    "    # Fallback: Check for standalone parentheses dates\n",
    "    if not job_data['Due_date']:\n",
    "        standalone_date_match = re.search(r'\\((\\d+)\\)', body)\n",
    "        if standalone_date_match:\n",
    "            full_number = standalone_date_match.group(1)\n",
    "            last_four = full_number[-4:]\n",
    "            if len(last_four) == 4:\n",
    "                job_data['Due_date'] = f\"{last_four[:2]}/{last_four[2:]}\"\n",
    "    \n",
    "    return job_data\n",
    "\n",
    "def count_emails_for_job_id(service, job_id):\n",
    "    \"\"\"Count emails containing the job ID in the inbox.\"\"\"\n",
    "    try:\n",
    "        query = f'\"{job_id}\"'  # Using quotes for exact matching\n",
    "        results = service.users().messages().list(\n",
    "            userId=\"me\",\n",
    "            q=query,\n",
    "            labelIds=['INBOX']\n",
    "        ).execute()\n",
    "        return results.get('resultSizeEstimate', 0)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error counting emails for {job_id}: {e}\")\n",
    "        return 0\n",
    "\n",
    "def process_job_ids(csv_path, service):\n",
    "    \"\"\"Process job IDs from CSV and update with email counts.\"\"\"\n",
    "    try:\n",
    "        with open(csv_path, 'r') as file:\n",
    "            reader = csv.DictReader(file)\n",
    "            rows = list(reader)\n",
    "            fieldnames = reader.fieldnames\n",
    "            \n",
    "        if 'No_of_emails' not in fieldnames:\n",
    "            fieldnames.append('No_of_emails')\n",
    "            for row in rows:\n",
    "                row['No_of_emails'] = '0'\n",
    "    \n",
    "        for row in rows:\n",
    "            job_id = row.get('Job ID', '').strip() or row.get('Job_ID', '').strip()\n",
    "            if job_id:\n",
    "                logging.info(f\"Searching for emails with job ID: {job_id}\")\n",
    "                email_count = count_emails_for_job_id(service, job_id)\n",
    "                row['No_of_emails'] = str(email_count)\n",
    "                logging.info(f\"Found {email_count} emails for {job_id}\")\n",
    "                time.sleep(0.5)  # Rate limiting\n",
    "    \n",
    "        with open(csv_path, 'w', newline='') as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(rows)\n",
    "    \n",
    "        logging.info(f\"Successfully updated CSV file: {csv_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing CSV file: {e}\")\n",
    "        raise\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to process both Gmail accounts\"\"\"\n",
    "    try:\n",
    "        # Initialize results table\n",
    "        results = []\n",
    "        \n",
    "        # Step 1: Authenticate with primary Gmail (extraction account)\n",
    "        primary_service = auto_authenticate_primary_gmail()\n",
    "        \n",
    "        # Step 2: Get recent emails from primary account\n",
    "        messages = get_recent_emails(primary_service, max_results=25)\n",
    "        \n",
    "        if not messages:\n",
    "            print(\"No emails found in primary inbox\")\n",
    "            return\n",
    "        \n",
    "        # Step 3: Process each email from primary account\n",
    "        print(\"\\n=== Processing Emails from Primary Account ===\")\n",
    "        for i, message in enumerate(messages, 1):\n",
    "            message_id = message['id']\n",
    "            print(f\"\\nProcessing Email #{i} (ID: {message_id})\")\n",
    "            \n",
    "            body = extract_email_body(primary_service, message_id)\n",
    "            \n",
    "            if body:\n",
    "                job_details = extract_job_details(body)\n",
    "                job_details['Email ID'] = message_id\n",
    "                results.append(job_details)\n",
    "                \n",
    "                print(f\"Title: {job_details['Title']}\")\n",
    "                print(f\"Job_ID: {job_details['Job_ID']}\")\n",
    "                print(f\"Due_date: {job_details['Due_date']}\")\n",
    "            else:\n",
    "                print(\"No body content could be extracted\")\n",
    "        \n",
    "        # Create and save results table\n",
    "        df = pd.DataFrame(results)\n",
    "        if not df.empty:\n",
    "            print(\"\\n=== Final Results ===\")\n",
    "            print(df[['Email ID', 'Title', 'Job_ID', 'Due_date']].to_string(index=False))\n",
    "            \n",
    "            csv_path = 'extracted_job_details.csv'\n",
    "            df.to_csv(csv_path, index=False)\n",
    "            print(f\"\\nResults saved to '{csv_path}'\")\n",
    "            \n",
    "            # Step 4: Authenticate with secondary Gmail (counting account)\n",
    "            secondary_service = auto_authenticate_secondary_gmail()\n",
    "            \n",
    "            # Step 5: Process CSV with secondary account\n",
    "            process_job_ids(csv_path, secondary_service)\n",
    "            print(\"\\nAdded email counts from secondary account to the CSV file\")\n",
    "        else:\n",
    "            print(\"\\nNo job details found in any emails\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78150757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Authenticating Primary Gmail Account ===\n",
      "Found primary token file, loading credentials...\n",
      "Primary credentials expired, refreshing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-22 16:21:55,532 - INFO - file_cache is only supported with oauth2client<4.0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building primary Gmail service...\n",
      "Primary Gmail service ready!\n",
      "\n",
      "=== Fetching 25 most recent emails ===\n",
      "Found 8 emails in inbox\n",
      "\n",
      "=== Processing Emails from Primary Account ===\n",
      "\n",
      "Processing Email #1 (ID: 1982174cf77fb3a3)\n",
      "Title: Hybrid/Local Analytics Engineer (MS Fabric/12+)\n",
      "Job_ID: GA-768025\n",
      "Due_date: 07/16\n",
      "\n",
      "Processing Email #2 (ID: 198217128c79c421)\n",
      "Title: Hybrid/Local Privacy Officer/Security Analyst (CISA/CIPM/CIPP)\n",
      "Job_ID: NC-768095\n",
      "Due_date: 07/24\n",
      "\n",
      "Processing Email #3 (ID: 198216d126f86664)\n",
      "Title: Remote Sitefinity Developer (12+)\n",
      "Job_ID: NC-765767\n",
      "Due_date: 06/25\n",
      "\n",
      "Processing Email #4 (ID: 198216c4da407075)\n",
      "Title: Remote Sitefinity Developer\n",
      "Job_ID: NC-765767\n",
      "Due_date: 07/21\n",
      "\n",
      "Processing Email #5 (ID: 1982169418497b56)\n",
      "Title: Remote/Local GIS Analyst (12+)\n",
      "Job_ID: NC-767681\n",
      "Due_date: 07/18\n",
      "\n",
      "Processing Email #6 (ID: 198216585c548259)\n",
      "Title: Hybrid/Local (Lansing ONLY) .NET Developer (12+)\n",
      "Job_ID: MI-144480\n",
      "Due_date: 07/21\n",
      "\n",
      "Processing Email #7 (ID: 1982135e4ab36a6a)\n",
      "Title: Hybrid/Local (Lansing 90 min) Full-stack Java Developer (15+)\n",
      "Job_ID: MI-144516\n",
      "Due_date: 07/22\n",
      "\n",
      "Processing Email #8 (ID: 1982128df901294a)\n",
      "Title: Hybrid/Local VA Govt BA\n",
      "Job_ID: VA-767445\n",
      "Due_date: 07/23\n",
      "\n",
      "=== Final Results ===\n",
      "        Email ID                                                          Title    Job_ID Due_date\n",
      "1982174cf77fb3a3                Hybrid/Local Analytics Engineer (MS Fabric/12+) GA-768025    07/16\n",
      "198217128c79c421 Hybrid/Local Privacy Officer/Security Analyst (CISA/CIPM/CIPP) NC-768095    07/24\n",
      "198216d126f86664                              Remote Sitefinity Developer (12+) NC-765767    06/25\n",
      "198216c4da407075                                    Remote Sitefinity Developer NC-765767    07/21\n",
      "1982169418497b56                                 Remote/Local GIS Analyst (12+) NC-767681    07/18\n",
      "198216585c548259               Hybrid/Local (Lansing ONLY) .NET Developer (12+) MI-144480    07/21\n",
      "1982135e4ab36a6a  Hybrid/Local (Lansing 90 min) Full-stack Java Developer (15+) MI-144516    07/22\n",
      "1982128df901294a                                        Hybrid/Local VA Govt BA VA-767445    07/23\n",
      "\n",
      "Results saved to 'duedates.csv'\n",
      "\n",
      "=== Authenticating Secondary Gmail Account ===\n",
      "Found secondary token file, loading credentials...\n",
      "Secondary credentials expired, refreshing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-22 16:22:00,190 - INFO - file_cache is only supported with oauth2client<4.0.0\n",
      "2025-07-22 16:22:00,207 - INFO - Searching for emails with job ID: GA-768025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building secondary Gmail service...\n",
      "Secondary Gmail service ready!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-22 16:22:00,784 - INFO - Found 3 emails for GA-768025\n",
      "2025-07-22 16:22:01,285 - INFO - Searching for emails with job ID: NC-768095\n",
      "2025-07-22 16:22:01,719 - INFO - Found 7 emails for NC-768095\n",
      "2025-07-22 16:22:02,232 - INFO - Searching for emails with job ID: NC-765767\n",
      "2025-07-22 16:22:02,664 - INFO - Found 6 emails for NC-765767\n",
      "2025-07-22 16:22:03,170 - INFO - Searching for emails with job ID: NC-765767\n",
      "2025-07-22 16:22:03,617 - INFO - Found 6 emails for NC-765767\n",
      "2025-07-22 16:22:04,119 - INFO - Searching for emails with job ID: NC-767681\n",
      "2025-07-22 16:22:04,549 - INFO - Found 6 emails for NC-767681\n",
      "2025-07-22 16:22:05,069 - INFO - Searching for emails with job ID: MI-144480\n",
      "2025-07-22 16:22:05,519 - INFO - Found 11 emails for MI-144480\n",
      "2025-07-22 16:22:06,034 - INFO - Searching for emails with job ID: MI-144516\n",
      "2025-07-22 16:22:06,467 - INFO - Found 1 emails for MI-144516\n",
      "2025-07-22 16:22:06,983 - INFO - Searching for emails with job ID: VA-767445\n",
      "2025-07-22 16:22:07,418 - INFO - Found 0 emails for VA-767445\n",
      "2025-07-22 16:22:07,920 - INFO - Successfully updated CSV file: duedates.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Added email counts from secondary account to the CSV file\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "import re\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from google.oauth2.credentials import Credentials\n",
    "from google.auth.transport.requests import Request\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from googleapiclient.discovery import build\n",
    "from base64 import urlsafe_b64decode\n",
    "import pytz\n",
    "import csv\n",
    "import time\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "def auto_authenticate_primary_gmail():\n",
    "    \"\"\"Authenticates with primary Gmail account (token.json)\"\"\"\n",
    "    print(\"\\n=== Authenticating Primary Gmail Account ===\")\n",
    "    creds = None\n",
    "    token_path = 'token.json'\n",
    "    SCOPES = ['https://www.googleapis.com/auth/gmail.readonly']\n",
    "    \n",
    "    if os.path.exists(token_path):\n",
    "        print(\"Found primary token file, loading credentials...\")\n",
    "        try:\n",
    "            creds = Credentials.from_authorized_user_file(token_path, SCOPES)\n",
    "            if creds.expired and creds.refresh_token:\n",
    "                print(\"Primary credentials expired, refreshing...\")\n",
    "                creds.refresh(Request())\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading primary credentials: {e}\")\n",
    "            creds = None\n",
    "    \n",
    "    if not creds or not creds.valid:\n",
    "        print(\"No valid primary credentials found, initiating OAuth flow...\")\n",
    "        try:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file('client.json', SCOPES)\n",
    "            creds = flow.run_local_server(port=8080)\n",
    "            token_data = json.loads(creds.to_json())\n",
    "            token_data['creation_time'] = datetime.now(pytz.UTC).isoformat()\n",
    "            with open(token_path, 'w') as token:\n",
    "                json.dump(token_data, token)\n",
    "            print(\"Primary authentication successful! Token saved.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Primary authentication failed: {e}\")\n",
    "            raise\n",
    "    \n",
    "    try:\n",
    "        print(\"Building primary Gmail service...\")\n",
    "        gmail_service = build('gmail', 'v1', credentials=creds)\n",
    "        print(\"Primary Gmail service ready!\")\n",
    "        return gmail_service\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to build primary Gmail service: {e}\")\n",
    "        raise\n",
    "\n",
    "def auto_authenticate_secondary_gmail():\n",
    "    \"\"\"Authenticates with secondary Gmail account (token1.json)\"\"\"\n",
    "    print(\"\\n=== Authenticating Secondary Gmail Account ===\")\n",
    "    creds = None\n",
    "    token_path = 'token1.json'\n",
    "    SCOPES = ['https://www.googleapis.com/auth/gmail.readonly']\n",
    "    \n",
    "    if os.path.exists(token_path):\n",
    "        print(\"Found secondary token file, loading credentials...\")\n",
    "        try:\n",
    "            creds = Credentials.from_authorized_user_file(token_path, SCOPES)\n",
    "            if creds.expired and creds.refresh_token:\n",
    "                print(\"Secondary credentials expired, refreshing...\")\n",
    "                creds.refresh(Request())\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading secondary credentials: {e}\")\n",
    "            creds = None\n",
    "    \n",
    "    if not creds or not creds.valid:\n",
    "        print(\"No valid secondary credentials found, initiating OAuth flow...\")\n",
    "        try:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file('client1.json', SCOPES)\n",
    "            creds = flow.run_local_server(port=8081)  # Different port than primary\n",
    "            token_data = json.loads(creds.to_json())\n",
    "            token_data['creation_time'] = datetime.now(pytz.UTC).isoformat()\n",
    "            with open(token_path, 'w') as token:\n",
    "                json.dump(token_data, token)\n",
    "            print(\"Secondary authentication successful! Token saved.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Secondary authentication failed: {e}\")\n",
    "            raise\n",
    "    \n",
    "    try:\n",
    "        print(\"Building secondary Gmail service...\")\n",
    "        gmail_service = build('gmail', 'v1', credentials=creds)\n",
    "        print(\"Secondary Gmail service ready!\")\n",
    "        return gmail_service\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to build secondary Gmail service: {e}\")\n",
    "        raise\n",
    "\n",
    "def get_recent_emails(service, max_results=30):\n",
    "    \"\"\"Fetches the most recent emails from the inbox\"\"\"\n",
    "    print(f\"\\n=== Fetching {max_results} most recent emails ===\")\n",
    "    try:\n",
    "        results = service.users().messages().list(\n",
    "            userId=\"me\",\n",
    "            labelIds=['INBOX'],\n",
    "            maxResults=max_results\n",
    "        ).execute()\n",
    "        messages = results.get('messages', [])\n",
    "        print(f\"Found {len(messages)} emails in inbox\")\n",
    "        return messages\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching emails: {e}\")\n",
    "        return []\n",
    "\n",
    "def decode_base64(data):\n",
    "    \"\"\"Decodes base64 email content with proper padding\"\"\"\n",
    "    missing_padding = len(data) % 4\n",
    "    if missing_padding:\n",
    "        data += '=' * (4 - missing_padding)\n",
    "    return urlsafe_b64decode(data)\n",
    "\n",
    "def extract_email_body(service, message_id):\n",
    "    \"\"\"Extracts and decodes the email body content\"\"\"\n",
    "    try:\n",
    "        msg = service.users().messages().get(\n",
    "            userId=\"me\",\n",
    "            id=message_id,\n",
    "            format='full'\n",
    "        ).execute()\n",
    "        \n",
    "        payload = msg.get('payload', {})\n",
    "        \n",
    "        # Extract body from payload\n",
    "        body = \"\"\n",
    "        if 'body' in payload and 'data' in payload['body']:\n",
    "            body = decode_base64(payload['body']['data']).decode('utf-8', errors='ignore')\n",
    "        elif 'parts' in payload:\n",
    "            for part in payload['parts']:\n",
    "                if part.get('mimeType') in ['text/plain', 'text/html']:\n",
    "                    if 'body' in part and 'data' in part['body']:\n",
    "                        body = decode_base64(part['body']['data']).decode('utf-8', errors='ignore')\n",
    "                        break\n",
    "                elif 'parts' in part:\n",
    "                    for subpart in part['parts']:\n",
    "                        if subpart.get('mimeType') in ['text/plain', 'text/html']:\n",
    "                            if 'body' in subpart and 'data' in subpart['body']:\n",
    "                                body = decode_base64(subpart['body']['data']).decode('utf-8', errors='ignore')\n",
    "                                break\n",
    "        \n",
    "        return body if body else None\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing email {message_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_job_details(body):\n",
    "    \"\"\"Enhanced version focusing on extracting dates from parentheses after Job IDs\"\"\"\n",
    "    job_data = {\n",
    "        'Title': None,\n",
    "        'Job_ID': None,\n",
    "        'Due_date': None\n",
    "    }\n",
    "    \n",
    "    if not body:\n",
    "        return job_data\n",
    "    \n",
    "    # Title extraction pattern\n",
    "    title_pattern = r'(?i)((?:Hybrid|Onsite|Remote)(?:\\s*/\\s*Local)?[^(]*\\(.*?\\)|(?:Hybrid|Onsite|Remote)(?:\\s*/\\s*Local)?.*?)(?=\\s+with\\s+|\\s*\\(|$|\\n)'\n",
    "    title_match = re.search(title_pattern, body)\n",
    "    if title_match:\n",
    "        job_data['Title'] = title_match.group(1).strip()\n",
    "    \n",
    "    # Job ID extraction\n",
    "    id_pattern = r'(?i)(?:job\\s*|req\\s*|position\\s*)?(?:id\\s*|#\\s*|num\\s*|number\\s*)?[:#]?\\s*([A-Za-z]{2,}-?\\d{3,})'\n",
    "    id_match = re.search(id_pattern, body)\n",
    "    if id_match:\n",
    "        job_id = id_match.group(1).strip()\n",
    "        job_data['Job_ID'] = job_id\n",
    "        \n",
    "        # Date extraction from parentheses after Job ID\n",
    "        date_match = re.search(\n",
    "            r'{}\\s*\\((\\d+)\\)'.format(re.escape(job_id)),\n",
    "            body\n",
    "        )\n",
    "        if date_match:\n",
    "            full_number = date_match.group(1)\n",
    "            last_four = full_number[-4:]\n",
    "            if len(last_four) == 4:\n",
    "                job_data['Due_date'] = f\"{last_four[:2]}/{last_four[2:]}\"\n",
    "    \n",
    "    # Fallback: If no Job ID found, try standalone ID pattern\n",
    "    else:\n",
    "        standalone_id = r'(?<!\\w)([A-Za-z]{2,}-?\\d{3,})(?!\\w)'\n",
    "        id_match = re.search(standalone_id, body)\n",
    "        if id_match:\n",
    "            job_data['Job_ID'] = id_match.group(1).strip()\n",
    "    \n",
    "    # Fallback: Check for standalone parentheses dates\n",
    "    if not job_data['Due_date']:\n",
    "        standalone_date_match = re.search(r'\\((\\d+)\\)', body)\n",
    "        if standalone_date_match:\n",
    "            full_number = standalone_date_match.group(1)\n",
    "            last_four = full_number[-4:]\n",
    "            if len(last_four) == 4:\n",
    "                job_data['Due_date'] = f\"{last_four[:2]}/{last_four[2:]}\"\n",
    "    \n",
    "    return job_data\n",
    "\n",
    "def count_emails_for_job_id(service, job_id):\n",
    "    \"\"\"Count emails containing the job ID in the inbox.\"\"\"\n",
    "    try:\n",
    "        query = f'\"{job_id}\"'  # Using quotes for exact matching\n",
    "        results = service.users().messages().list(\n",
    "            userId=\"me\",\n",
    "            q=query,\n",
    "            labelIds=['INBOX']\n",
    "        ).execute()\n",
    "        return results.get('resultSizeEstimate', 0)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error counting emails for {job_id}: {e}\")\n",
    "        return 0\n",
    "\n",
    "def process_job_ids(csv_path, service):\n",
    "    \"\"\"Process job IDs from CSV and update with email counts.\"\"\"\n",
    "    try:\n",
    "        with open(csv_path, 'r') as file:\n",
    "            reader = csv.DictReader(file)\n",
    "            rows = list(reader)\n",
    "            fieldnames = reader.fieldnames\n",
    "            \n",
    "        if 'No_of_emails' not in fieldnames:\n",
    "            fieldnames.append('No_of_emails')\n",
    "            for row in rows:\n",
    "                row['No_of_emails'] = '0'\n",
    "    \n",
    "        for row in rows:\n",
    "            job_id = row.get('Job ID', '').strip() or row.get('Job_ID', '').strip()\n",
    "            if job_id:\n",
    "                logging.info(f\"Searching for emails with job ID: {job_id}\")\n",
    "                email_count = count_emails_for_job_id(service, job_id)\n",
    "                row['No_of_emails'] = str(email_count)\n",
    "                logging.info(f\"Found {email_count} emails for {job_id}\")\n",
    "                time.sleep(0.5)  # Rate limiting\n",
    "    \n",
    "        with open(csv_path, 'w', newline='') as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(rows)\n",
    "    \n",
    "        logging.info(f\"Successfully updated CSV file: {csv_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing CSV file: {e}\")\n",
    "        raise\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to process both Gmail accounts\"\"\"\n",
    "    try:\n",
    "        # Initialize results table\n",
    "        results = []\n",
    "        \n",
    "        # Step 1: Authenticate with primary Gmail (extraction account)\n",
    "        primary_service = auto_authenticate_primary_gmail()\n",
    "        \n",
    "        # Step 2: Get recent emails from primary account\n",
    "        messages = get_recent_emails(primary_service, max_results=25)\n",
    "        \n",
    "        if not messages:\n",
    "            print(\"No emails found in primary inbox\")\n",
    "            return\n",
    "        \n",
    "        # Step 3: Process each email from primary account\n",
    "        print(\"\\n=== Processing Emails from Primary Account ===\")\n",
    "        for i, message in enumerate(messages, 1):\n",
    "            message_id = message['id']\n",
    "            print(f\"\\nProcessing Email #{i} (ID: {message_id})\")\n",
    "            \n",
    "            body = extract_email_body(primary_service, message_id)\n",
    "            \n",
    "            if body:\n",
    "                job_details = extract_job_details(body)\n",
    "                job_details['Email ID'] = message_id\n",
    "                results.append(job_details)\n",
    "                \n",
    "                print(f\"Title: {job_details['Title']}\")\n",
    "                print(f\"Job_ID: {job_details['Job_ID']}\")\n",
    "                print(f\"Due_date: {job_details['Due_date']}\")\n",
    "            else:\n",
    "                print(\"No body content could be extracted\")\n",
    "        \n",
    "        # Create and save results table\n",
    "        df = pd.DataFrame(results)\n",
    "        if not df.empty:\n",
    "            print(\"\\n=== Final Results ===\")\n",
    "            print(df[['Email ID', 'Title', 'Job_ID', 'Due_date']].to_string(index=False))\n",
    "            \n",
    "            csv_path = 'duedates.csv'\n",
    "            df.to_csv(csv_path, index=False)\n",
    "            print(f\"\\nResults saved to '{csv_path}'\")\n",
    "            \n",
    "            # Step 4: Authenticate with secondary Gmail (counting account)\n",
    "            secondary_service = auto_authenticate_secondary_gmail()\n",
    "            \n",
    "            # Step 5: Process CSV with secondary account\n",
    "            process_job_ids(csv_path, secondary_service)\n",
    "            print(\"\\nAdded email counts from secondary account to the CSV file\")\n",
    "        else:\n",
    "            print(\"\\nNo job details found in any emails\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7a5c40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8eb1998f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Authenticating Primary Gmail Account ===\n",
      "Found primary token file, loading credentials...\n",
      "Primary credentials expired, refreshing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 15:08:25,578 - INFO - file_cache is only supported with oauth2client<4.0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building primary Gmail service...\n",
      "Primary Gmail service ready!\n",
      "\n",
      "=== Fetching 25 most recent emails ===\n",
      "Found 12 emails in inbox\n",
      "\n",
      "=== Processing Emails from Primary Account ===\n",
      "\n",
      "Processing Email #1 (ID: 198361443a2622e8)\n",
      "Title: None\n",
      "Job_ID: None\n",
      "Due_date: None\n",
      "\n",
      "Processing Email #2 (ID: 1983613a0e5db70b)\n",
      "Title: None\n",
      "Job_ID: fa1853842\n",
      "Due_date: None\n",
      "\n",
      "Processing Email #3 (ID: 19835e64d5ff260b)\n",
      "Title: None\n",
      "Job_ID: llC804\n",
      "Due_date: None\n",
      "\n",
      "Processing Email #4 (ID: 198322c9c74dd4d2)\n",
      "Title: None\n",
      "Job_ID: ea257\n",
      "Due_date: None\n",
      "\n",
      "Processing Email #5 (ID: 1982174cf77fb3a3)\n",
      "Title: Hybrid/Local Analytics Engineer (MS Fabric/12+)\n",
      "Job_ID: GA-768025\n",
      "Due_date: 07/16\n",
      "\n",
      "Processing Email #6 (ID: 198217128c79c421)\n",
      "Title: Hybrid/Local Privacy Officer/Security Analyst (CISA/CIPM/CIPP)\n",
      "Job_ID: NC-768095\n",
      "Due_date: 07/24\n",
      "\n",
      "Processing Email #7 (ID: 198216d126f86664)\n",
      "Title: Remote Sitefinity Developer (12+)\n",
      "Job_ID: NC-765767\n",
      "Due_date: 06/25\n",
      "\n",
      "Processing Email #8 (ID: 198216c4da407075)\n",
      "Title: Remote Sitefinity Developer\n",
      "Job_ID: NC-765767\n",
      "Due_date: 07/21\n",
      "\n",
      "Processing Email #9 (ID: 1982169418497b56)\n",
      "Title: Remote/Local GIS Analyst (12+)\n",
      "Job_ID: NC-767681\n",
      "Due_date: 07/18\n",
      "\n",
      "Processing Email #10 (ID: 198216585c548259)\n",
      "Title: Hybrid/Local (Lansing ONLY) .NET Developer (12+)\n",
      "Job_ID: MI-144480\n",
      "Due_date: 07/21\n",
      "\n",
      "Processing Email #11 (ID: 1982135e4ab36a6a)\n",
      "Title: Hybrid/Local (Lansing 90 min) Full-stack Java Developer (15+)\n",
      "Job_ID: MI-144516\n",
      "Due_date: 07/22\n",
      "\n",
      "Processing Email #12 (ID: 1982128df901294a)\n",
      "Title: Hybrid/Local VA Govt BA\n",
      "Job_ID: VA-767445\n",
      "Due_date: 07/23\n",
      "\n",
      "Removed existing file: 'duedates.csv'\n",
      "\n",
      "=== Authenticating Secondary Gmail Account ===\n",
      "Found secondary token file, loading credentials...\n",
      "Secondary credentials expired, refreshing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 15:08:32,180 - INFO - file_cache is only supported with oauth2client<4.0.0\n",
      "2025-07-23 15:08:32,185 - INFO - Searching for emails with job ID: GA-768025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building secondary Gmail service...\n",
      "Secondary Gmail service ready!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 15:08:32,729 - INFO - Found 3 emails for GA-768025\n",
      "2025-07-23 15:08:33,231 - INFO - Searching for emails with job ID: NC-768095\n",
      "2025-07-23 15:08:33,663 - INFO - Found 8 emails for NC-768095\n",
      "2025-07-23 15:08:34,174 - INFO - Searching for emails with job ID: NC-765767\n",
      "2025-07-23 15:08:34,595 - INFO - Found 6 emails for NC-765767\n",
      "2025-07-23 15:08:35,124 - INFO - Searching for emails with job ID: NC-765767\n",
      "2025-07-23 15:08:35,554 - INFO - Found 6 emails for NC-765767\n",
      "2025-07-23 15:08:36,058 - INFO - Searching for emails with job ID: NC-767681\n",
      "2025-07-23 15:08:36,494 - INFO - Found 6 emails for NC-767681\n",
      "2025-07-23 15:08:37,006 - INFO - Searching for emails with job ID: MI-144480\n",
      "2025-07-23 15:08:37,444 - INFO - Found 14 emails for MI-144480\n",
      "2025-07-23 15:08:37,956 - INFO - Searching for emails with job ID: MI-144516\n",
      "2025-07-23 15:08:38,389 - INFO - Found 1 emails for MI-144516\n",
      "2025-07-23 15:08:38,891 - INFO - Searching for emails with job ID: VA-767445\n",
      "2025-07-23 15:08:39,326 - INFO - Found 0 emails for VA-767445\n",
      "2025-07-23 15:08:39,830 - INFO - Successfully updated CSV file: duedates.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "                  FINAL RESULTS                   \n",
      "==================================================\n",
      "+----+----------------------------------------------------------------+-----------+------------+----------------+\n",
      "|    | Title                                                          | Job_ID    | Due_date   | No_of_emails   |\n",
      "+====+================================================================+===========+============+================+\n",
      "| 0  | Hybrid/Local Analytics Engineer (MS Fabric/12+)                | GA-768025 | 07/16      | 3              |\n",
      "+----+----------------------------------------------------------------+-----------+------------+----------------+\n",
      "| 1  | Hybrid/Local Privacy Officer/Security Analyst (CISA/CIPM/CIPP) | NC-768095 | 07/24      | 8              |\n",
      "+----+----------------------------------------------------------------+-----------+------------+----------------+\n",
      "| 2  | Remote Sitefinity Developer (12+)                              | NC-765767 | 06/25      | 6              |\n",
      "+----+----------------------------------------------------------------+-----------+------------+----------------+\n",
      "| 3  | Remote Sitefinity Developer                                    | NC-765767 | 07/21      | 6              |\n",
      "+----+----------------------------------------------------------------+-----------+------------+----------------+\n",
      "| 4  | Remote/Local GIS Analyst (12+)                                 | NC-767681 | 07/18      | 6              |\n",
      "+----+----------------------------------------------------------------+-----------+------------+----------------+\n",
      "| 5  | Hybrid/Local (Lansing ONLY) .NET Developer (12+)               | MI-144480 | 07/21      | 14             |\n",
      "+----+----------------------------------------------------------------+-----------+------------+----------------+\n",
      "| 6  | Hybrid/Local (Lansing 90 min) Full-stack Java Developer (15+)  | MI-144516 | 07/22      | 1              |\n",
      "+----+----------------------------------------------------------------+-----------+------------+----------------+\n",
      "| 7  | Hybrid/Local VA Govt BA                                        | VA-767445 | 07/23      | 0              |\n",
      "+----+----------------------------------------------------------------+-----------+------------+----------------+\n",
      "==================================================\n",
      "\n",
      "Results saved to 'duedates.csv'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "import re\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from google.oauth2.credentials import Credentials\n",
    "from google.auth.transport.requests import Request\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from googleapiclient.discovery import build\n",
    "from base64 import urlsafe_b64decode\n",
    "import pytz\n",
    "import csv\n",
    "import time\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "def auto_authenticate_primary_gmail():\n",
    "    \"\"\"Authenticates with primary Gmail account (token.json)\"\"\"\n",
    "    print(\"\\n=== Authenticating Primary Gmail Account ===\")\n",
    "    creds = None\n",
    "    token_path = 'token.json'\n",
    "    SCOPES = ['https://www.googleapis.com/auth/gmail.readonly']\n",
    "    \n",
    "    if os.path.exists(token_path):\n",
    "        print(\"Found primary token file, loading credentials...\")\n",
    "        try:\n",
    "            creds = Credentials.from_authorized_user_file(token_path, SCOPES)\n",
    "            if creds.expired and creds.refresh_token:\n",
    "                print(\"Primary credentials expired, refreshing...\")\n",
    "                creds.refresh(Request())\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading primary credentials: {e}\")\n",
    "            creds = None\n",
    "    \n",
    "    if not creds or not creds.valid:\n",
    "        print(\"No valid primary credentials found, initiating OAuth flow...\")\n",
    "        try:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file('client.json', SCOPES)\n",
    "            creds = flow.run_local_server(port=8080)\n",
    "            token_data = json.loads(creds.to_json())\n",
    "            token_data['creation_time'] = datetime.now(pytz.UTC).isoformat()\n",
    "            with open(token_path, 'w') as token:\n",
    "                json.dump(token_data, token)\n",
    "            print(\"Primary authentication successful! Token saved.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Primary authentication failed: {e}\")\n",
    "            raise\n",
    "    \n",
    "    try:\n",
    "        print(\"Building primary Gmail service...\")\n",
    "        gmail_service = build('gmail', 'v1', credentials=creds)\n",
    "        print(\"Primary Gmail service ready!\")\n",
    "        return gmail_service\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to build primary Gmail service: {e}\")\n",
    "        raise\n",
    "\n",
    "def auto_authenticate_secondary_gmail():\n",
    "    \"\"\"Authenticates with secondary Gmail account (token1.json)\"\"\"\n",
    "    print(\"\\n=== Authenticating Secondary Gmail Account ===\")\n",
    "    creds = None\n",
    "    token_path = 'token1.json'\n",
    "    SCOPES = ['https://www.googleapis.com/auth/gmail.readonly']\n",
    "    \n",
    "    if os.path.exists(token_path):\n",
    "        print(\"Found secondary token file, loading credentials...\")\n",
    "        try:\n",
    "            creds = Credentials.from_authorized_user_file(token_path, SCOPES)\n",
    "            if creds.expired and creds.refresh_token:\n",
    "                print(\"Secondary credentials expired, refreshing...\")\n",
    "                creds.refresh(Request())\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading secondary credentials: {e}\")\n",
    "            creds = None\n",
    "    \n",
    "    if not creds or not creds.valid:\n",
    "        print(\"No valid secondary credentials found, initiating OAuth flow...\")\n",
    "        try:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file('client1.json', SCOPES)\n",
    "            creds = flow.run_local_server(port=8081)  # Different port than primary\n",
    "            token_data = json.loads(creds.to_json())\n",
    "            token_data['creation_time'] = datetime.now(pytz.UTC).isoformat()\n",
    "            with open(token_path, 'w') as token:\n",
    "                json.dump(token_data, token)\n",
    "            print(\"Secondary authentication successful! Token saved.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Secondary authentication failed: {e}\")\n",
    "            raise\n",
    "    \n",
    "    try:\n",
    "        print(\"Building secondary Gmail service...\")\n",
    "        gmail_service = build('gmail', 'v1', credentials=creds)\n",
    "        print(\"Secondary Gmail service ready!\")\n",
    "        return gmail_service\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to build secondary Gmail service: {e}\")\n",
    "        raise\n",
    "\n",
    "def get_recent_emails(service, max_results=30):\n",
    "    \"\"\"Fetches the most recent emails from the inbox\"\"\"\n",
    "    print(f\"\\n=== Fetching {max_results} most recent emails ===\")\n",
    "    try:\n",
    "        results = service.users().messages().list(\n",
    "            userId=\"me\",\n",
    "            labelIds=['INBOX'],\n",
    "            maxResults=max_results\n",
    "        ).execute()\n",
    "        messages = results.get('messages', [])\n",
    "        print(f\"Found {len(messages)} emails in inbox\")\n",
    "        return messages\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching emails: {e}\")\n",
    "        return []\n",
    "\n",
    "def decode_base64(data):\n",
    "    \"\"\"Decodes base64 email content with proper padding\"\"\"\n",
    "    missing_padding = len(data) % 4\n",
    "    if missing_padding:\n",
    "        data += '=' * (4 - missing_padding)\n",
    "    return urlsafe_b64decode(data)\n",
    "\n",
    "def extract_email_body(service, message_id):\n",
    "    \"\"\"Extracts and decodes the email body content\"\"\"\n",
    "    try:\n",
    "        msg = service.users().messages().get(\n",
    "            userId=\"me\",\n",
    "            id=message_id,\n",
    "            format='full'\n",
    "        ).execute()\n",
    "        \n",
    "        payload = msg.get('payload', {})\n",
    "        \n",
    "        # Extract body from payload\n",
    "        body = \"\"\n",
    "        if 'body' in payload and 'data' in payload['body']:\n",
    "            body = decode_base64(payload['body']['data']).decode('utf-8', errors='ignore')\n",
    "        elif 'parts' in payload:\n",
    "            for part in payload['parts']:\n",
    "                if part.get('mimeType') in ['text/plain', 'text/html']:\n",
    "                    if 'body' in part and 'data' in part['body']:\n",
    "                        body = decode_base64(part['body']['data']).decode('utf-8', errors='ignore')\n",
    "                        break\n",
    "                elif 'parts' in part:\n",
    "                    for subpart in part['parts']:\n",
    "                        if subpart.get('mimeType') in ['text/plain', 'text/html']:\n",
    "                            if 'body' in subpart and 'data' in subpart['body']:\n",
    "                                body = decode_base64(subpart['body']['data']).decode('utf-8', errors='ignore')\n",
    "                                break\n",
    "        \n",
    "        return body if body else None\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing email {message_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_job_details(body):\n",
    "    \"\"\"Enhanced version focusing on extracting dates from parentheses after Job IDs\"\"\"\n",
    "    job_data = {\n",
    "        'Title': None,\n",
    "        'Job_ID': None,\n",
    "        'Due_date': None\n",
    "    }\n",
    "    \n",
    "    if not body:\n",
    "        return job_data\n",
    "    \n",
    "    # Title extraction pattern\n",
    "    title_pattern = r'(?i)((?:Hybrid|Onsite|Remote)(?:\\s*/\\s*Local)?[^(]*\\(.*?\\)|(?:Hybrid|Onsite|Remote)(?:\\s*/\\s*Local)?.*?)(?=\\s+with\\s+|\\s*\\(|$|\\n)'\n",
    "    title_match = re.search(title_pattern, body)\n",
    "    if title_match:\n",
    "        job_data['Title'] = title_match.group(1).strip()\n",
    "    \n",
    "    # Job ID extraction\n",
    "    id_pattern = r'(?i)(?:job\\s*|req\\s*|position\\s*)?(?:id\\s*|#\\s*|num\\s*|number\\s*)?[:#]?\\s*([A-Za-z]{2,}-?\\d{3,})'\n",
    "    id_match = re.search(id_pattern, body)\n",
    "    if id_match:\n",
    "        job_id = id_match.group(1).strip()\n",
    "        job_data['Job_ID'] = job_id\n",
    "        \n",
    "        # Date extraction from parentheses after Job ID\n",
    "        date_match = re.search(\n",
    "            r'{}\\s*\\((\\d+)\\)'.format(re.escape(job_id)),\n",
    "            body\n",
    "        )\n",
    "        if date_match:\n",
    "            full_number = date_match.group(1)\n",
    "            last_four = full_number[-4:]\n",
    "            if len(last_four) == 4:\n",
    "                job_data['Due_date'] = f\"{last_four[:2]}/{last_four[2:]}\"\n",
    "    \n",
    "    # Fallback: If no Job ID found, try standalone ID pattern\n",
    "    else:\n",
    "        standalone_id = r'(?<!\\w)([A-Za-z]{2,}-?\\d{3,})(?!\\w)'\n",
    "        id_match = re.search(standalone_id, body)\n",
    "        if id_match:\n",
    "            job_data['Job_ID'] = id_match.group(1).strip()\n",
    "    \n",
    "    # Fallback: Check for standalone parentheses dates\n",
    "    if not job_data['Due_date']:\n",
    "        standalone_date_match = re.search(r'\\((\\d+)\\)', body)\n",
    "        if standalone_date_match:\n",
    "            full_number = standalone_date_match.group(1)\n",
    "            last_four = full_number[-4:]\n",
    "            if len(last_four) == 4:\n",
    "                job_data['Due_date'] = f\"{last_four[:2]}/{last_four[2:]}\"\n",
    "    \n",
    "    return job_data\n",
    "\n",
    "def count_emails_for_job_id(service, job_id):\n",
    "    \"\"\"Count emails containing the job ID in the inbox.\"\"\"\n",
    "    try:\n",
    "        query = f'\"{job_id}\"'  # Using quotes for exact matching\n",
    "        results = service.users().messages().list(\n",
    "            userId=\"me\",\n",
    "            q=query,\n",
    "            labelIds=['INBOX']\n",
    "        ).execute()\n",
    "        return results.get('resultSizeEstimate', 0)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error counting emails for {job_id}: {e}\")\n",
    "        return 0\n",
    "\n",
    "def process_job_ids(csv_path, service):\n",
    "    \"\"\"Process job IDs from CSV and update with email counts.\"\"\"\n",
    "    try:\n",
    "        with open(csv_path, 'r') as file:\n",
    "            reader = csv.DictReader(file)\n",
    "            rows = list(reader)\n",
    "            fieldnames = reader.fieldnames\n",
    "            \n",
    "        if 'No_of_emails' not in fieldnames:\n",
    "            fieldnames.append('No_of_emails')\n",
    "            for row in rows:\n",
    "                row['No_of_emails'] = '0'\n",
    "    \n",
    "        for row in rows:\n",
    "            job_id = row.get('Job ID', '').strip() or row.get('Job_ID', '').strip()\n",
    "            if job_id:\n",
    "                logging.info(f\"Searching for emails with job ID: {job_id}\")\n",
    "                email_count = count_emails_for_job_id(service, job_id)\n",
    "                row['No_of_emails'] = str(email_count)\n",
    "                logging.info(f\"Found {email_count} emails for {job_id}\")\n",
    "                time.sleep(0.5)  # Rate limiting\n",
    "    \n",
    "        with open(csv_path, 'w', newline='') as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(rows)\n",
    "    \n",
    "        logging.info(f\"Successfully updated CSV file: {csv_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing CSV file: {e}\")\n",
    "        raise\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to process both Gmail accounts\"\"\"\n",
    "    try:\n",
    "        # Initialize results table\n",
    "        results = []\n",
    "        \n",
    "        # Step 1: Authenticate with primary Gmail (extraction account)\n",
    "        primary_service = auto_authenticate_primary_gmail()\n",
    "        \n",
    "        # Step 2: Get recent emails from primary account\n",
    "        messages = get_recent_emails(primary_service, max_results=25)\n",
    "        \n",
    "        if not messages:\n",
    "            print(\"No emails found in primary inbox\")\n",
    "            return\n",
    "        \n",
    "        # Step 3: Process each email from primary account\n",
    "        print(\"\\n=== Processing Emails from Primary Account ===\")\n",
    "        for i, message in enumerate(messages, 1):\n",
    "            message_id = message['id']\n",
    "            print(f\"\\nProcessing Email #{i} (ID: {message_id})\")\n",
    "            \n",
    "            body = extract_email_body(primary_service, message_id)\n",
    "            \n",
    "            if body:\n",
    "                job_details = extract_job_details(body)\n",
    "                job_details['Email ID'] = message_id  # Temporarily keep for processing\n",
    "                results.append(job_details)\n",
    "                \n",
    "                print(f\"Title: {job_details['Title']}\")\n",
    "                print(f\"Job_ID: {job_details['Job_ID']}\")\n",
    "                print(f\"Due_date: {job_details['Due_date']}\")\n",
    "            else:\n",
    "                print(\"No body content could be extracted\")\n",
    "        \n",
    "        # Create DataFrame and filter rows where Title is missing\n",
    "        df = pd.DataFrame(results)\n",
    "        df = df.dropna(subset=['Title'])  # Remove rows with missing Title\n",
    "        \n",
    "        if not df.empty:\n",
    "            csv_path = 'duedates.csv'\n",
    "            \n",
    "            # Remove existing CSV file if it exists\n",
    "            if os.path.exists(csv_path):\n",
    "                os.remove(csv_path)\n",
    "                print(f\"\\nRemoved existing file: '{csv_path}'\")\n",
    "            \n",
    "            # Remove 'Email ID' column before saving\n",
    "            df = df.drop(columns=['Email ID'], errors='ignore')\n",
    "            \n",
    "            # Save initial CSV (without No_of_emails column yet)\n",
    "            df.to_csv(csv_path, index=False)\n",
    "            \n",
    "            # Step 4: Authenticate with secondary Gmail (counting account)\n",
    "            secondary_service = auto_authenticate_secondary_gmail()\n",
    "            \n",
    "            # Step 5: Process CSV with secondary account to add No_of_emails\n",
    "            process_job_ids(csv_path, secondary_service)\n",
    "            \n",
    "            # Reload the final CSV to display all columns\n",
    "            final_df = pd.read_csv(csv_path)\n",
    "            \n",
    "            # Display final table with all columns (properly formatted)\n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            print(\"FINAL RESULTS\".center(50))\n",
    "            print(\"=\"*50)\n",
    "            \n",
    "            # Configure pandas display options for better formatting\n",
    "            pd.set_option('display.max_colwidth', 40)\n",
    "            pd.set_option('display.width', 120)\n",
    "            pd.set_option('display.colheader_justify', 'center')\n",
    "            \n",
    "            # Print the formatted table\n",
    "            print(final_df.to_markdown(tablefmt=\"grid\", stralign=\"left\", numalign=\"left\"))\n",
    "            print(\"=\"*50)\n",
    "            \n",
    "            print(f\"\\nResults saved to '{csv_path}'\")\n",
    "        else:\n",
    "            print(\"\\nNo valid job details found (all rows filtered out)\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2c230e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc185f7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2eeb2384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Authenticating Primary Gmail Account ===\n",
      "Found primary token file, loading credentials...\n",
      "Primary credentials expired, refreshing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-24 12:19:24,169 - INFO - file_cache is only supported with oauth2client<4.0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building primary Gmail service...\n",
      "Primary Gmail service ready!\n",
      "\n",
      "=== Fetching 25 most recent emails ===\n",
      "Found 22 emails in inbox\n",
      "\n",
      "=== Processing Emails from Primary Account ===\n",
      "\n",
      "Processing Email #1 (ID: 198393575cacd4f9)\n",
      "Title: Onsite/Local Compliance Officer\n",
      "Job_ID: FL-773154\n",
      "Due_date: 07/29\n",
      "\n",
      "Processing Email #2 (ID: 198392a579360a53)\n",
      "Title: Onsite/Local Compliance Officer\n",
      "Job_ID: FL-773156\n",
      "Due_date: 07/29\n",
      "\n",
      "Processing Email #3 (ID: 198392635986247c)\n",
      "Title: Hybrid/Local .NET Developer (10+)\n",
      "Job_ID: SC-7571\n",
      "Due_date: 07/30\n",
      "\n",
      "Processing Email #4 (ID: 1983917093cec522)\n",
      "Title: Onsite/Local Compliance Officer\n",
      "Job_ID: FL-773155\n",
      "Due_date: 07/29\n",
      "\n",
      "Processing Email #5 (ID: 19838198f3582674)\n",
      "Title: Hybrid/Local Security Architect (CISSP/CCSP/Azure/AWS/GCP/15+)\n",
      "Job_ID: VA-767227\n",
      "Due_date: 07/28\n",
      "\n",
      "Processing Email #6 (ID: 19837c6c0c189722)\n",
      "Title: Hybrid/Local .NET/C# Developer (15+)\n",
      "Job_ID: VA-768268\n",
      "Due_date: 07/28\n",
      "\n",
      "Processing Email #7 (ID: 19837c14ea90518f)\n",
      "Title: Onsite/Local Desktop Support Technician (travel/In-person IV)\n",
      "Job_ID: VA-768094\n",
      "Due_date: 07/28\n",
      "\n",
      "Processing Email #8 (ID: 19837806ca88a50d)\n",
      "Title: Hybrid/Local Instructional Designer\n",
      "Job_ID: NC-764016\n",
      "Due_date: 08/01\n",
      "\n",
      "Processing Email #9 (ID: 198377de718e7e4c)\n",
      "Title: Hybrid/Local Instructional Designer\n",
      "Job_ID: NC-766519\n",
      "Due_date: 08/01\n",
      "\n",
      "Processing Email #10 (ID: 1983773f508be940)\n",
      "Title: Hybrid/Local Curam Developer (certification must)\n",
      "Job_ID: SC-7566\n",
      "Due_date: 07/28\n",
      "\n",
      "Processing Email #11 (ID: 198361443a2622e8)\n",
      "Title: None\n",
      "Job_ID: None\n",
      "Due_date: None\n",
      "\n",
      "Processing Email #12 (ID: 1983613a0e5db70b)\n",
      "Title: None\n",
      "Job_ID: None\n",
      "Due_date: None\n",
      "\n",
      "Processing Email #13 (ID: 19835e64d5ff260b)\n",
      "Title: None\n",
      "Job_ID: HS-45403856\n",
      "Due_date: None\n",
      "\n",
      "Processing Email #14 (ID: 198322c9c74dd4d2)\n",
      "Title: None\n",
      "Job_ID: ID1565481562\n",
      "Due_date: None\n",
      "\n",
      "Processing Email #15 (ID: 1982174cf77fb3a3)\n",
      "Title: Hybrid/Local Analytics Engineer (MS Fabric/12+)\n",
      "Job_ID: GA-768025\n",
      "Due_date: 07/16\n",
      "\n",
      "Processing Email #16 (ID: 198217128c79c421)\n",
      "Title: Hybrid/Local Privacy Officer/Security Analyst (CISA/CIPM/CIPP)\n",
      "Job_ID: NC-768095\n",
      "Due_date: 07/24\n",
      "\n",
      "Processing Email #17 (ID: 198216d126f86664)\n",
      "Title: Remote Sitefinity Developer (12+)\n",
      "Job_ID: NC-765767\n",
      "Due_date: 06/25\n",
      "\n",
      "Processing Email #18 (ID: 198216c4da407075)\n",
      "Title: Remote Sitefinity Developer\n",
      "Job_ID: NC-765767\n",
      "Due_date: 07/21\n",
      "\n",
      "Processing Email #19 (ID: 1982169418497b56)\n",
      "Title: Remote/Local GIS Analyst (12+)\n",
      "Job_ID: NC-767681\n",
      "Due_date: 07/18\n",
      "\n",
      "Processing Email #20 (ID: 198216585c548259)\n",
      "Title: Hybrid/Local (Lansing ONLY) .NET Developer (12+)\n",
      "Job_ID: MI-144480\n",
      "Due_date: 07/21\n",
      "\n",
      "Processing Email #21 (ID: 1982135e4ab36a6a)\n",
      "Title: Hybrid/Local (Lansing 90 min) Full-stack Java Developer (15+)\n",
      "Job_ID: MI-144516\n",
      "Due_date: 07/22\n",
      "\n",
      "Processing Email #22 (ID: 1982128df901294a)\n",
      "Title: Hybrid/Local VA Govt BA\n",
      "Job_ID: VA-767445\n",
      "Due_date: 07/23\n",
      "\n",
      "=== Authenticating Secondary Gmail Account ===\n",
      "Found secondary token file, loading credentials...\n",
      "Secondary credentials expired, refreshing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-24 12:19:36,491 - INFO - file_cache is only supported with oauth2client<4.0.0\n",
      "2025-07-24 12:19:36,498 - INFO - Searching for emails with job ID: FL-773154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building secondary Gmail service...\n",
      "Secondary Gmail service ready!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-24 12:19:37,033 - INFO - Found 0 emails for FL-773154\n",
      "2025-07-24 12:19:37,547 - INFO - Searching for emails with job ID: FL-773156\n",
      "2025-07-24 12:19:37,966 - INFO - Found 0 emails for FL-773156\n",
      "2025-07-24 12:19:38,467 - INFO - Searching for emails with job ID: SC-7571\n",
      "2025-07-24 12:19:38,903 - INFO - Found 0 emails for SC-7571\n",
      "2025-07-24 12:19:39,407 - INFO - Searching for emails with job ID: FL-773155\n",
      "2025-07-24 12:19:39,829 - INFO - Found 0 emails for FL-773155\n",
      "2025-07-24 12:19:40,344 - INFO - Searching for emails with job ID: VA-767227\n",
      "2025-07-24 12:19:40,808 - INFO - Found 4 emails for VA-767227\n",
      "2025-07-24 12:19:41,317 - INFO - Searching for emails with job ID: VA-768268\n",
      "2025-07-24 12:19:41,755 - INFO - Found 14 emails for VA-768268\n",
      "2025-07-24 12:19:42,260 - INFO - Searching for emails with job ID: VA-768094\n",
      "2025-07-24 12:19:42,690 - INFO - Found 0 emails for VA-768094\n",
      "2025-07-24 12:19:43,203 - INFO - Searching for emails with job ID: NC-764016\n",
      "2025-07-24 12:19:43,644 - INFO - Found 0 emails for NC-764016\n",
      "2025-07-24 12:19:44,155 - INFO - Searching for emails with job ID: NC-766519\n",
      "2025-07-24 12:19:44,589 - INFO - Found 0 emails for NC-766519\n",
      "2025-07-24 12:19:45,092 - INFO - Searching for emails with job ID: SC-7566\n",
      "2025-07-24 12:19:45,518 - INFO - Found 0 emails for SC-7566\n",
      "2025-07-24 12:19:46,028 - INFO - Searching for emails with job ID: GA-768025\n",
      "2025-07-24 12:19:46,465 - INFO - Found 3 emails for GA-768025\n",
      "2025-07-24 12:19:46,972 - INFO - Searching for emails with job ID: NC-768095\n",
      "2025-07-24 12:19:47,398 - INFO - Found 8 emails for NC-768095\n",
      "2025-07-24 12:19:47,912 - INFO - Searching for emails with job ID: NC-765767\n",
      "2025-07-24 12:19:48,359 - INFO - Found 6 emails for NC-765767\n",
      "2025-07-24 12:19:48,875 - INFO - Searching for emails with job ID: NC-765767\n",
      "2025-07-24 12:19:49,302 - INFO - Found 6 emails for NC-765767\n",
      "2025-07-24 12:19:49,953 - INFO - Searching for emails with job ID: NC-767681\n",
      "2025-07-24 12:19:50,565 - INFO - Found 6 emails for NC-767681\n",
      "2025-07-24 12:19:51,078 - INFO - Searching for emails with job ID: MI-144480\n",
      "2025-07-24 12:19:51,518 - INFO - Found 14 emails for MI-144480\n",
      "2025-07-24 12:19:52,027 - INFO - Searching for emails with job ID: MI-144516\n",
      "2025-07-24 12:19:52,464 - INFO - Found 1 emails for MI-144516\n",
      "2025-07-24 12:19:52,978 - INFO - Searching for emails with job ID: VA-767445\n",
      "2025-07-24 12:19:53,391 - INFO - Found 0 emails for VA-767445\n",
      "2025-07-24 12:19:53,919 - INFO - Successfully updated CSV file: temp_duedates.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "                                           FINAL RESULTS                                            \n",
      "====================================================================================================\n",
      "    +----------------------------------------------------------------+-----------+------------+----------------+\n",
      "    | Title                                                          | Job_ID    | Due_date   | No_of_emails   |\n",
      "    +================================================================+===========+============+================+\n",
      "    | Onsite/Local Compliance Officer                                | FL-773154 | 07/29      | 0              |\n",
      "    +----------------------------------------------------------------+-----------+------------+----------------+\n",
      "    | Onsite/Local Compliance Officer                                | FL-773156 | 07/29      | 0              |\n",
      "    +----------------------------------------------------------------+-----------+------------+----------------+\n",
      "    | Hybrid/Local .NET Developer (10+)                              | SC-7571   | 07/30      | 0              |\n",
      "    +----------------------------------------------------------------+-----------+------------+----------------+\n",
      "    | Onsite/Local Compliance Officer                                | FL-773155 | 07/29      | 0              |\n",
      "    +----------------------------------------------------------------+-----------+------------+----------------+\n",
      "    | Hybrid/Local Security Architect (CISSP/CCSP/Azure/AWS/GCP/15+) | VA-767227 | 07/28      | 4              |\n",
      "    +----------------------------------------------------------------+-----------+------------+----------------+\n",
      "    | Hybrid/Local .NET/C# Developer (15+)                           | VA-768268 | 07/28      | 14             |\n",
      "    +----------------------------------------------------------------+-----------+------------+----------------+\n",
      "    | Onsite/Local Desktop Support Technician (travel/In-person IV)  | VA-768094 | 07/28      | 0              |\n",
      "    +----------------------------------------------------------------+-----------+------------+----------------+\n",
      "    | Hybrid/Local Instructional Designer                            | NC-764016 | 08/01      | 0              |\n",
      "    +----------------------------------------------------------------+-----------+------------+----------------+\n",
      "    | Hybrid/Local Instructional Designer                            | NC-766519 | 08/01      | 0              |\n",
      "    +----------------------------------------------------------------+-----------+------------+----------------+\n",
      "    | Hybrid/Local Curam Developer (certification must)              | SC-7566   | 07/28      | 0              |\n",
      "    +----------------------------------------------------------------+-----------+------------+----------------+\n",
      "    | Hybrid/Local Analytics Engineer (MS Fabric/12+)                | GA-768025 | 07/16      | 3              |\n",
      "    +----------------------------------------------------------------+-----------+------------+----------------+\n",
      "    | Hybrid/Local Privacy Officer/Security Analyst (CISA/CIPM/CIPP) | NC-768095 | 07/24      | 8              |\n",
      "    +----------------------------------------------------------------+-----------+------------+----------------+\n",
      "    | Remote Sitefinity Developer (12+)                              | NC-765767 | 06/25      | 6              |\n",
      "    +----------------------------------------------------------------+-----------+------------+----------------+\n",
      "    | Remote Sitefinity Developer                                    | NC-765767 | 07/21      | 6              |\n",
      "    +----------------------------------------------------------------+-----------+------------+----------------+\n",
      "    | Remote/Local GIS Analyst (12+)                                 | NC-767681 | 07/18      | 6              |\n",
      "    +----------------------------------------------------------------+-----------+------------+----------------+\n",
      "    | Hybrid/Local (Lansing ONLY) .NET Developer (12+)               | MI-144480 | 07/21      | 14             |\n",
      "    +----------------------------------------------------------------+-----------+------------+----------------+\n",
      "    | Hybrid/Local (Lansing 90 min) Full-stack Java Developer (15+)  | MI-144516 | 07/22      | 1              |\n",
      "    +----------------------------------------------------------------+-----------+------------+----------------+\n",
      "    | Hybrid/Local VA Govt BA                                        | VA-767445 | 07/23      | 0              |\n",
      "    +----------------------------------------------------------------+-----------+------------+----------------+\n",
      "====================================================================================================\n",
      "\n",
      "Final results saved to 'duedates_formatted.xlsx'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "import re\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from google.oauth2.credentials import Credentials\n",
    "from google.auth.transport.requests import Request\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from googleapiclient.discovery import build\n",
    "from base64 import urlsafe_b64decode\n",
    "import pytz\n",
    "import csv\n",
    "import time\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "def auto_authenticate_primary_gmail():\n",
    "    \"\"\"Authenticates with primary Gmail account (token.json)\"\"\"\n",
    "    print(\"\\n=== Authenticating Primary Gmail Account ===\")\n",
    "    creds = None\n",
    "    token_path = 'token.json'\n",
    "    SCOPES = ['https://www.googleapis.com/auth/gmail.readonly']\n",
    "    \n",
    "    if os.path.exists(token_path):\n",
    "        print(\"Found primary token file, loading credentials...\")\n",
    "        try:\n",
    "            creds = Credentials.from_authorized_user_file(token_path, SCOPES)\n",
    "            if creds.expired and creds.refresh_token:\n",
    "                print(\"Primary credentials expired, refreshing...\")\n",
    "                creds.refresh(Request())\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading primary credentials: {e}\")\n",
    "            creds = None\n",
    "    \n",
    "    if not creds or not creds.valid:\n",
    "        print(\"No valid primary credentials found, initiating OAuth flow...\")\n",
    "        try:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file('client.json', SCOPES)\n",
    "            creds = flow.run_local_server(port=8080)\n",
    "            token_data = json.loads(creds.to_json())\n",
    "            token_data['creation_time'] = datetime.now(pytz.UTC).isoformat()\n",
    "            with open(token_path, 'w') as token:\n",
    "                json.dump(token_data, token)\n",
    "            print(\"Primary authentication successful! Token saved.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Primary authentication failed: {e}\")\n",
    "            raise\n",
    "    \n",
    "    try:\n",
    "        print(\"Building primary Gmail service...\")\n",
    "        gmail_service = build('gmail', 'v1', credentials=creds)\n",
    "        print(\"Primary Gmail service ready!\")\n",
    "        return gmail_service\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to build primary Gmail service: {e}\")\n",
    "        raise\n",
    "\n",
    "def auto_authenticate_secondary_gmail():\n",
    "    \"\"\"Authenticates with secondary Gmail account (token1.json)\"\"\"\n",
    "    print(\"\\n=== Authenticating Secondary Gmail Account ===\")\n",
    "    creds = None\n",
    "    token_path = 'token1.json'\n",
    "    SCOPES = ['https://www.googleapis.com/auth/gmail.readonly']\n",
    "    \n",
    "    if os.path.exists(token_path):\n",
    "        print(\"Found secondary token file, loading credentials...\")\n",
    "        try:\n",
    "            creds = Credentials.from_authorized_user_file(token_path, SCOPES)\n",
    "            if creds.expired and creds.refresh_token:\n",
    "                print(\"Secondary credentials expired, refreshing...\")\n",
    "                creds.refresh(Request())\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading secondary credentials: {e}\")\n",
    "            creds = None\n",
    "    \n",
    "    if not creds or not creds.valid:\n",
    "        print(\"No valid secondary credentials found, initiating OAuth flow...\")\n",
    "        try:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file('client1.json', SCOPES)\n",
    "            creds = flow.run_local_server(port=8081)  # Different port than primary\n",
    "            token_data = json.loads(creds.to_json())\n",
    "            token_data['creation_time'] = datetime.now(pytz.UTC).isoformat()\n",
    "            with open(token_path, 'w') as token:\n",
    "                json.dump(token_data, token)\n",
    "            print(\"Secondary authentication successful! Token saved.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Secondary authentication failed: {e}\")\n",
    "            raise\n",
    "    \n",
    "    try:\n",
    "        print(\"Building secondary Gmail service...\")\n",
    "        gmail_service = build('gmail', 'v1', credentials=creds)\n",
    "        print(\"Secondary Gmail service ready!\")\n",
    "        return gmail_service\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to build secondary Gmail service: {e}\")\n",
    "        raise\n",
    "\n",
    "def get_recent_emails(service, max_results=30):\n",
    "    \"\"\"Fetches the most recent emails from the inbox\"\"\"\n",
    "    print(f\"\\n=== Fetching {max_results} most recent emails ===\")\n",
    "    try:\n",
    "        results = service.users().messages().list(\n",
    "            userId=\"me\",\n",
    "            labelIds=['INBOX'],\n",
    "            maxResults=max_results\n",
    "        ).execute()\n",
    "        messages = results.get('messages', [])\n",
    "        print(f\"Found {len(messages)} emails in inbox\")\n",
    "        return messages\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching emails: {e}\")\n",
    "        return []\n",
    "\n",
    "def decode_base64(data):\n",
    "    \"\"\"Decodes base64 email content with proper padding\"\"\"\n",
    "    missing_padding = len(data) % 4\n",
    "    if missing_padding:\n",
    "        data += '=' * (4 - missing_padding)\n",
    "    return urlsafe_b64decode(data)\n",
    "\n",
    "def extract_email_body(service, message_id):\n",
    "    \"\"\"Extracts and decodes the email body content\"\"\"\n",
    "    try:\n",
    "        msg = service.users().messages().get(\n",
    "            userId=\"me\",\n",
    "            id=message_id,\n",
    "            format='full'\n",
    "        ).execute()\n",
    "        \n",
    "        payload = msg.get('payload', {})\n",
    "        \n",
    "        # Extract body from payload\n",
    "        body = \"\"\n",
    "        if 'body' in payload and 'data' in payload['body']:\n",
    "            body = decode_base64(payload['body']['data']).decode('utf-8', errors='ignore')\n",
    "        elif 'parts' in payload:\n",
    "            for part in payload['parts']:\n",
    "                if part.get('mimeType') in ['text/plain', 'text/html']:\n",
    "                    if 'body' in part and 'data' in part['body']:\n",
    "                        body = decode_base64(part['body']['data']).decode('utf-8', errors='ignore')\n",
    "                        break\n",
    "                elif 'parts' in part:\n",
    "                    for subpart in part['parts']:\n",
    "                        if subpart.get('mimeType') in ['text/plain', 'text/html']:\n",
    "                            if 'body' in subpart and 'data' in subpart['body']:\n",
    "                                body = decode_base64(subpart['body']['data']).decode('utf-8', errors='ignore')\n",
    "                                break\n",
    "        \n",
    "        return body if body else None\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing email {message_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_job_details(body):\n",
    "    \"\"\"Enhanced version specifically targeting Job ID fields in emails\"\"\"\n",
    "    job_data = {\n",
    "        'Title': None,\n",
    "        'Job_ID': None,\n",
    "        'Due_date': None\n",
    "    }\n",
    "    \n",
    "    if not body:\n",
    "        return job_data\n",
    "    \n",
    "    # Title extraction pattern (unchanged)\n",
    "    title_pattern = r'(?i)((?:Hybrid|Onsite|Remote)(?:\\s*/\\s*Local)?[^(]*\\(.*?\\)|(?:Hybrid|Onsite|Remote)(?:\\s*/\\s*Local)?.*?)(?=\\s+with\\s+|\\s*\\(|$|\\n)'\n",
    "    title_match = re.search(title_pattern, body)\n",
    "    if title_match:\n",
    "        job_data['Title'] = title_match.group(1).strip()\n",
    "    \n",
    "    # NEW: Focused Job ID extraction - looks for specific patterns indicating a Job ID field\n",
    "    job_id_patterns = [\n",
    "        r'(?i)(?:job\\s*|req\\s*|position\\s*)?(?:id\\s*|#\\s*|num\\s*|number\\s*)[:=\\s]*([A-Za-z]{2,}-?\\d{3,})',  # \"Job ID: VA-123456\"\n",
    "        r'(?i)(?:job\\s*|req\\s*|position\\s*)?(?:id\\s*|#\\s*|num\\s*|number\\s*)[:=\\s]*([A-Za-z]{2,}\\s*\\d{3,})',  # \"Job ID VA 123456\"\n",
    "        r'(?i)\\b(?:job\\s*|req\\s*|position\\s*)?(?:id\\s*|#\\s*|num\\s*|number\\s*)\\b\\s*[:-]?\\s*([A-Za-z]{2,}-?\\d{3,})',  # \"Job-ID:VA-123456\"\n",
    "        r'(?<!\\w)([A-Za-z]{2,}-?\\d{3,})(?!\\w)',  # Standalone ID as last resort\n",
    "    ]\n",
    "    \n",
    "    for pattern in job_id_patterns:\n",
    "        id_match = re.search(pattern, body)\n",
    "        if id_match:\n",
    "            job_id = id_match.group(1).strip()\n",
    "            # Clean up the job ID (remove spaces, normalize format)\n",
    "            job_id = re.sub(r'\\s+', '', job_id)  # Remove any spaces\n",
    "            job_id = job_id.upper()  # Convert to uppercase for consistency\n",
    "            job_data['Job_ID'] = job_id\n",
    "            break\n",
    "    \n",
    "    # Date extraction only if we found a Job ID\n",
    "    if job_data['Job_ID']:\n",
    "        # Look for dates in parentheses after Job ID\n",
    "        date_match = re.search(\n",
    "            r'{}\\s*\\((\\d+)\\)'.format(re.escape(job_data['Job_ID'])),\n",
    "            body\n",
    "        )\n",
    "        if date_match:\n",
    "            full_number = date_match.group(1)\n",
    "            last_four = full_number[-4:]\n",
    "            if len(last_four) == 4:\n",
    "                job_data['Due_date'] = f\"{last_four[:2]}/{last_four[2:]}\"\n",
    "    \n",
    "    return job_data\n",
    "\n",
    "def count_emails_for_job_id(service, job_id):\n",
    "    \"\"\"Count emails containing the job ID in the inbox.\"\"\"\n",
    "    try:\n",
    "        query = f'\"{job_id}\"'  # Using quotes for exact matching\n",
    "        results = service.users().messages().list(\n",
    "            userId=\"me\",\n",
    "            q=query,\n",
    "            labelIds=['INBOX']\n",
    "        ).execute()\n",
    "        return results.get('resultSizeEstimate', 0)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error counting emails for {job_id}: {e}\")\n",
    "        return 0\n",
    "\n",
    "def process_job_ids(csv_path, service):\n",
    "    \"\"\"Process job IDs from CSV and update with email counts.\"\"\"\n",
    "    try:\n",
    "        with open(csv_path, 'r') as file:\n",
    "            reader = csv.DictReader(file)\n",
    "            rows = list(reader)\n",
    "            fieldnames = reader.fieldnames\n",
    "            \n",
    "        if 'No_of_emails' not in fieldnames:\n",
    "            fieldnames.append('No_of_emails')\n",
    "            for row in rows:\n",
    "                row['No_of_emails'] = '0'\n",
    "    \n",
    "        for row in rows:\n",
    "            job_id = row.get('Job ID', '').strip() or row.get('Job_ID', '').strip()\n",
    "            if job_id:\n",
    "                logging.info(f\"Searching for emails with job ID: {job_id}\")\n",
    "                email_count = count_emails_for_job_id(service, job_id)\n",
    "                row['No_of_emails'] = str(email_count)\n",
    "                logging.info(f\"Found {email_count} emails for {job_id}\")\n",
    "                time.sleep(0.5)  # Rate limiting\n",
    "    \n",
    "        with open(csv_path, 'w', newline='') as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(rows)\n",
    "    \n",
    "        logging.info(f\"Successfully updated CSV file: {csv_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing CSV file: {e}\")\n",
    "        raise\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to process both Gmail accounts\"\"\"\n",
    "    try:\n",
    "        # Initialize results table\n",
    "        results = []\n",
    "        \n",
    "        # Step 1: Authenticate with primary Gmail (extraction account)\n",
    "        primary_service = auto_authenticate_primary_gmail()\n",
    "        \n",
    "        # Step 2: Get recent emails from primary account\n",
    "        messages = get_recent_emails(primary_service, max_results=25)\n",
    "        \n",
    "        if not messages:\n",
    "            print(\"No emails found in primary inbox\")\n",
    "            return\n",
    "        \n",
    "        # Step 3: Process each email from primary account\n",
    "        print(\"\\n=== Processing Emails from Primary Account ===\")\n",
    "        for i, message in enumerate(messages, 1):\n",
    "            message_id = message['id']\n",
    "            print(f\"\\nProcessing Email #{i} (ID: {message_id})\")\n",
    "            \n",
    "            body = extract_email_body(primary_service, message_id)\n",
    "            \n",
    "            if body:\n",
    "                job_details = extract_job_details(body)\n",
    "                job_details['Email ID'] = message_id  # Temporarily keep for processing\n",
    "                results.append(job_details)\n",
    "                \n",
    "                print(f\"Title: {job_details['Title']}\")\n",
    "                print(f\"Job_ID: {job_details['Job_ID']}\")\n",
    "                print(f\"Due_date: {job_details['Due_date']}\")\n",
    "            else:\n",
    "                print(\"No body content could be extracted\")\n",
    "        \n",
    "        # Create DataFrame and filter rows where Title is missing\n",
    "        df = pd.DataFrame(results)\n",
    "        df = df.dropna(subset=['Title'])  # Remove rows with missing Title\n",
    "        \n",
    "        if not df.empty:\n",
    "            # Remove 'Email ID' column\n",
    "            df = df.drop(columns=['Email ID'], errors='ignore')\n",
    "            \n",
    "            # Create temporary CSV for secondary account processing\n",
    "            temp_csv = 'temp_duedates.csv'\n",
    "            df.to_csv(temp_csv, index=False)\n",
    "            \n",
    "            # Step 4: Authenticate with secondary Gmail (counting account)\n",
    "            secondary_service = auto_authenticate_secondary_gmail()\n",
    "            \n",
    "            # Step 5: Process CSV with secondary account to add No_of_emails\n",
    "            process_job_ids(temp_csv, secondary_service)\n",
    "            \n",
    "            # Load the final data\n",
    "            final_df = pd.read_csv(temp_csv)\n",
    "            \n",
    "            # Remove temporary CSV\n",
    "            os.remove(temp_csv)\n",
    "            \n",
    "            # Display final table\n",
    "            print(\"\\n\" + \"=\"*100)\n",
    "            print(\"FINAL RESULTS\".center(100))\n",
    "            print(\"=\"*100)\n",
    "            \n",
    "            # Configure display options\n",
    "            pd.set_option('display.max_colwidth', 40)\n",
    "            pd.set_option('display.width', 120)\n",
    "            pd.set_option('display.colheader_justify', 'center')\n",
    "            \n",
    "            # Create formatted table\n",
    "            table = final_df.to_markdown(\n",
    "                tablefmt=\"grid\",\n",
    "                stralign=\"left\",\n",
    "                numalign=\"left\",\n",
    "                index=False\n",
    "            )\n",
    "            \n",
    "            # Add left margin to each line\n",
    "            margined_table = [f\"    {line}\" for line in table.split('\\n')]\n",
    "            print('\\n'.join(margined_table))\n",
    "            print(\"=\"*100)\n",
    "            \n",
    "            # Save to Excel with formatting\n",
    "            excel_path = 'duedates_formatted.xlsx'\n",
    "            try:\n",
    "                import xlsxwriter\n",
    "                writer = pd.ExcelWriter(excel_path, engine='xlsxwriter')\n",
    "                final_df.to_excel(writer, index=False, sheet_name='Job Details')\n",
    "                \n",
    "                workbook = writer.book\n",
    "                worksheet = writer.sheets['Job Details']\n",
    "                \n",
    "                # Formatting\n",
    "                header_format = workbook.add_format({\n",
    "                    'bold': True,\n",
    "                    'text_wrap': True,\n",
    "                    'valign': 'top',\n",
    "                    'align': 'center',\n",
    "                    'border': 1\n",
    "                })\n",
    "                \n",
    "                cell_format = workbook.add_format({\n",
    "                    'text_wrap': True,\n",
    "                    'valign': 'top',\n",
    "                    'align': 'left',\n",
    "                    'border': 1\n",
    "                })\n",
    "                \n",
    "                # Apply formatting\n",
    "                for col_num, value in enumerate(final_df.columns.values):\n",
    "                    worksheet.write(0, col_num, value, header_format)\n",
    "                \n",
    "                for row in range(1, len(final_df)+1):\n",
    "                    for col in range(len(final_df.columns)):\n",
    "                        worksheet.write(row, col, str(final_df.iloc[row-1, col]), cell_format)\n",
    "                \n",
    "                # Auto-adjust column widths\n",
    "                for i, col in enumerate(final_df.columns):\n",
    "                    max_len = max((\n",
    "                        final_df[col].astype(str).map(len).max(),\n",
    "                        len(col)\n",
    "                    )) + 2\n",
    "                    worksheet.set_column(i, i, max_len)\n",
    "                \n",
    "                writer.close()\n",
    "                print(f\"\\nFinal results saved to '{excel_path}'\")\n",
    "                \n",
    "            except ImportError:\n",
    "                print(\"\\nError: xlsxwriter not installed - cannot create Excel file\")\n",
    "                print(\"Install with: pip install xlsxwriter\")\n",
    "                # Fallback to CSV if Excel fails\n",
    "                csv_path = 'duedates.csv'\n",
    "                final_df.to_csv(csv_path, index=False)\n",
    "                print(f\"Results saved to '{csv_path}' instead\")\n",
    "            \n",
    "        else:\n",
    "            print(\"\\nNo valid job details found (all rows filtered out)\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c683792a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "684c9159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Authenticating Primary Gmail Account ===\n",
      "Found primary token file, loading credentials...\n",
      "Primary credentials expired, refreshing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-24 12:07:14,996 - INFO - file_cache is only supported with oauth2client<4.0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building primary Gmail service...\n",
      "Primary Gmail service ready!\n",
      "\n",
      "=== Fetching 25 most recent emails ===\n",
      "Found 22 emails in inbox\n",
      "\n",
      "=== Processing Emails from Primary Account ===\n",
      "\n",
      "Processing Email #1 (ID: 198393575cacd4f9)\n",
      "Title: Onsite/Local Compliance Officer\n",
      "Job_ID: FL-773154\n",
      "Due_date: 07/29\n",
      "\n",
      "Processing Email #2 (ID: 198392a579360a53)\n",
      "Title: Onsite/Local Compliance Officer\n",
      "Job_ID: FL-773156\n",
      "Due_date: 07/29\n",
      "\n",
      "Processing Email #3 (ID: 198392635986247c)\n",
      "Title: Hybrid/Local .NET Developer (10+)\n",
      "Job_ID: SC-7571\n",
      "Due_date: 07/30\n",
      "\n",
      "Processing Email #4 (ID: 1983917093cec522)\n",
      "Title: Onsite/Local Compliance Officer\n",
      "Job_ID: FL-773155\n",
      "Due_date: 07/29\n",
      "\n",
      "Processing Email #5 (ID: 19838198f3582674)\n",
      "Title: Hybrid/Local Security Architect (CISSP/CCSP/Azure/AWS/GCP/15+)\n",
      "Job_ID: VA-767227\n",
      "Due_date: 07/28\n",
      "\n",
      "Processing Email #6 (ID: 19837c6c0c189722)\n",
      "Title: Hybrid/Local .NET/C# Developer (15+)\n",
      "Job_ID: VA-768268\n",
      "Due_date: 07/28\n",
      "\n",
      "Processing Email #7 (ID: 19837c14ea90518f)\n",
      "Title: Onsite/Local Desktop Support Technician (travel/In-person IV)\n",
      "Job_ID: VA-768094\n",
      "Due_date: 07/28\n",
      "\n",
      "Processing Email #8 (ID: 19837806ca88a50d)\n",
      "Title: Hybrid/Local Instructional Designer\n",
      "Job_ID: NC-764016\n",
      "Due_date: 08/01\n",
      "\n",
      "Processing Email #9 (ID: 198377de718e7e4c)\n",
      "Title: Hybrid/Local Instructional Designer\n",
      "Job_ID: NC-766519\n",
      "Due_date: 08/01\n",
      "\n",
      "Processing Email #10 (ID: 1983773f508be940)\n",
      "Title: Hybrid/Local Curam Developer (certification must)\n",
      "Job_ID: SC-7566\n",
      "Due_date: 07/28\n",
      "\n",
      "Processing Email #11 (ID: 198361443a2622e8)\n",
      "Title: None\n",
      "Job_ID: None\n",
      "Due_date: None\n",
      "\n",
      "Processing Email #12 (ID: 1983613a0e5db70b)\n",
      "Title: None\n",
      "Job_ID: None\n",
      "Due_date: None\n",
      "\n",
      "Processing Email #13 (ID: 19835e64d5ff260b)\n",
      "Title: None\n",
      "Job_ID: HS-45403856\n",
      "Due_date: None\n",
      "\n",
      "Processing Email #14 (ID: 198322c9c74dd4d2)\n",
      "Title: None\n",
      "Job_ID: ID1565481562\n",
      "Due_date: None\n",
      "\n",
      "Processing Email #15 (ID: 1982174cf77fb3a3)\n",
      "Title: Hybrid/Local Analytics Engineer (MS Fabric/12+)\n",
      "Job_ID: GA-768025\n",
      "Due_date: 07/16\n",
      "\n",
      "Processing Email #16 (ID: 198217128c79c421)\n",
      "Title: Hybrid/Local Privacy Officer/Security Analyst (CISA/CIPM/CIPP)\n",
      "Job_ID: NC-768095\n",
      "Due_date: 07/24\n",
      "\n",
      "Processing Email #17 (ID: 198216d126f86664)\n",
      "Title: Remote Sitefinity Developer (12+)\n",
      "Job_ID: NC-765767\n",
      "Due_date: 06/25\n",
      "\n",
      "Processing Email #18 (ID: 198216c4da407075)\n",
      "Title: Remote Sitefinity Developer\n",
      "Job_ID: NC-765767\n",
      "Due_date: 07/21\n",
      "\n",
      "Processing Email #19 (ID: 1982169418497b56)\n",
      "Title: Remote/Local GIS Analyst (12+)\n",
      "Job_ID: NC-767681\n",
      "Due_date: 07/18\n",
      "\n",
      "Processing Email #20 (ID: 198216585c548259)\n",
      "Title: Hybrid/Local (Lansing ONLY) .NET Developer (12+)\n",
      "Job_ID: MI-144480\n",
      "Due_date: 07/21\n",
      "\n",
      "Processing Email #21 (ID: 1982135e4ab36a6a)\n",
      "Title: Hybrid/Local (Lansing 90 min) Full-stack Java Developer (15+)\n",
      "Job_ID: MI-144516\n",
      "Due_date: 07/22\n",
      "\n",
      "Processing Email #22 (ID: 1982128df901294a)\n",
      "Title: Hybrid/Local VA Govt BA\n",
      "Job_ID: VA-767445\n",
      "Due_date: 07/23\n",
      "\n",
      "=== Authenticating Secondary Gmail Account ===\n",
      "Found secondary token file, loading credentials...\n",
      "Secondary credentials expired, refreshing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-24 12:07:27,609 - INFO - file_cache is only supported with oauth2client<4.0.0\n",
      "2025-07-24 12:07:27,627 - INFO - Searching for emails with job ID: FL-773154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building secondary Gmail service...\n",
      "Secondary Gmail service ready!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-24 12:07:28,223 - INFO - Found 0 emails for FL-773154\n",
      "2025-07-24 12:07:28,738 - INFO - Searching for emails with job ID: FL-773156\n",
      "2025-07-24 12:07:29,169 - INFO - Found 0 emails for FL-773156\n",
      "2025-07-24 12:07:29,676 - INFO - Searching for emails with job ID: SC-7571\n",
      "2025-07-24 12:07:30,110 - INFO - Found 0 emails for SC-7571\n",
      "2025-07-24 12:07:30,621 - INFO - Searching for emails with job ID: FL-773155\n",
      "2025-07-24 12:07:31,061 - INFO - Found 0 emails for FL-773155\n",
      "2025-07-24 12:07:31,576 - INFO - Searching for emails with job ID: VA-767227\n",
      "2025-07-24 12:07:32,135 - INFO - Found 4 emails for VA-767227\n",
      "2025-07-24 12:07:32,647 - INFO - Searching for emails with job ID: VA-768268\n",
      "2025-07-24 12:07:33,149 - INFO - Found 14 emails for VA-768268\n",
      "2025-07-24 12:07:33,652 - INFO - Searching for emails with job ID: VA-768094\n",
      "2025-07-24 12:07:34,110 - INFO - Found 0 emails for VA-768094\n",
      "2025-07-24 12:07:34,618 - INFO - Searching for emails with job ID: NC-764016\n",
      "2025-07-24 12:07:35,196 - INFO - Found 0 emails for NC-764016\n",
      "2025-07-24 12:07:35,707 - INFO - Searching for emails with job ID: NC-766519\n",
      "2025-07-24 12:07:36,173 - INFO - Found 0 emails for NC-766519\n",
      "2025-07-24 12:07:36,677 - INFO - Searching for emails with job ID: SC-7566\n",
      "2025-07-24 12:07:37,137 - INFO - Found 0 emails for SC-7566\n",
      "2025-07-24 12:07:37,643 - INFO - Searching for emails with job ID: GA-768025\n",
      "2025-07-24 12:07:38,082 - INFO - Found 3 emails for GA-768025\n",
      "2025-07-24 12:07:38,583 - INFO - Searching for emails with job ID: NC-768095\n",
      "2025-07-24 12:07:39,086 - INFO - Found 8 emails for NC-768095\n",
      "2025-07-24 12:07:39,596 - INFO - Searching for emails with job ID: NC-765767\n",
      "2025-07-24 12:07:40,124 - INFO - Found 6 emails for NC-765767\n",
      "2025-07-24 12:07:40,629 - INFO - Searching for emails with job ID: NC-765767\n",
      "2025-07-24 12:07:41,144 - INFO - Found 6 emails for NC-765767\n",
      "2025-07-24 12:07:41,649 - INFO - Searching for emails with job ID: NC-767681\n",
      "2025-07-24 12:07:42,164 - INFO - Found 6 emails for NC-767681\n",
      "2025-07-24 12:07:42,673 - INFO - Searching for emails with job ID: MI-144480\n",
      "2025-07-24 12:07:43,188 - INFO - Found 14 emails for MI-144480\n",
      "2025-07-24 12:07:43,695 - INFO - Searching for emails with job ID: MI-144516\n",
      "2025-07-24 12:07:44,208 - INFO - Found 1 emails for MI-144516\n",
      "2025-07-24 12:07:44,715 - INFO - Searching for emails with job ID: VA-767445\n",
      "2025-07-24 12:07:45,159 - INFO - Found 0 emails for VA-767445\n",
      "2025-07-24 12:07:45,662 - INFO - Successfully updated CSV file: temp_duedates.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "                                           FINAL RESULTS                                            \n",
      "====================================================================================================\n",
      "    +----------------------------------------------------------------+-----------+------------+----------------+\n",
      "    | Title                                                          | Job_ID    | Due_date   | No_of_emails   |\n",
      "    +================================================================+===========+============+================+\n",
      "    | Onsite/Local Compliance Officer                                | FL-773154 | 07/29      | 0              |\n",
      "    +----------------------------------------------------------------+-----------+------------+----------------+\n",
      "    | Onsite/Local Compliance Officer                                | FL-773156 | 07/29      | 0              |\n",
      "    +----------------------------------------------------------------+-----------+------------+----------------+\n",
      "    | Hybrid/Local .NET Developer (10+)                              | SC-7571   | 07/30      | 0              |\n",
      "    +----------------------------------------------------------------+-----------+------------+----------------+\n",
      "    | Onsite/Local Compliance Officer                                | FL-773155 | 07/29      | 0              |\n",
      "    +----------------------------------------------------------------+-----------+------------+----------------+\n",
      "    | Hybrid/Local Security Architect (CISSP/CCSP/Azure/AWS/GCP/15+) | VA-767227 | 07/28      | 4              |\n",
      "    +----------------------------------------------------------------+-----------+------------+----------------+\n",
      "    | Hybrid/Local .NET/C# Developer (15+)                           | VA-768268 | 07/28      | 14             |\n",
      "    +----------------------------------------------------------------+-----------+------------+----------------+\n",
      "    | Onsite/Local Desktop Support Technician (travel/In-person IV)  | VA-768094 | 07/28      | 0              |\n",
      "    +----------------------------------------------------------------+-----------+------------+----------------+\n",
      "    | Hybrid/Local Instructional Designer                            | NC-764016 | 08/01      | 0              |\n",
      "    +----------------------------------------------------------------+-----------+------------+----------------+\n",
      "    | Hybrid/Local Instructional Designer                            | NC-766519 | 08/01      | 0              |\n",
      "    +----------------------------------------------------------------+-----------+------------+----------------+\n",
      "    | Hybrid/Local Curam Developer (certification must)              | SC-7566   | 07/28      | 0              |\n",
      "    +----------------------------------------------------------------+-----------+------------+----------------+\n",
      "    | Hybrid/Local Analytics Engineer (MS Fabric/12+)                | GA-768025 | 07/16      | 3              |\n",
      "    +----------------------------------------------------------------+-----------+------------+----------------+\n",
      "    | Hybrid/Local Privacy Officer/Security Analyst (CISA/CIPM/CIPP) | NC-768095 | 07/24      | 8              |\n",
      "    +----------------------------------------------------------------+-----------+------------+----------------+\n",
      "    | Remote Sitefinity Developer (12+)                              | NC-765767 | 06/25      | 6              |\n",
      "    +----------------------------------------------------------------+-----------+------------+----------------+\n",
      "    | Remote Sitefinity Developer                                    | NC-765767 | 07/21      | 6              |\n",
      "    +----------------------------------------------------------------+-----------+------------+----------------+\n",
      "    | Remote/Local GIS Analyst (12+)                                 | NC-767681 | 07/18      | 6              |\n",
      "    +----------------------------------------------------------------+-----------+------------+----------------+\n",
      "    | Hybrid/Local (Lansing ONLY) .NET Developer (12+)               | MI-144480 | 07/21      | 14             |\n",
      "    +----------------------------------------------------------------+-----------+------------+----------------+\n",
      "    | Hybrid/Local (Lansing 90 min) Full-stack Java Developer (15+)  | MI-144516 | 07/22      | 1              |\n",
      "    +----------------------------------------------------------------+-----------+------------+----------------+\n",
      "    | Hybrid/Local VA Govt BA                                        | VA-767445 | 07/23      | 0              |\n",
      "    +----------------------------------------------------------------+-----------+------------+----------------+\n",
      "====================================================================================================\n",
      "\n",
      "Final results saved to 'duedates_formatted.xlsx'\n",
      "\n",
      "Results successfully emailed to nandini.ka12345@gmail.com\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "import re\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from google.oauth2.credentials import Credentials\n",
    "from google.auth.transport.requests import Request\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from googleapiclient.discovery import build\n",
    "from base64 import urlsafe_b64decode\n",
    "import pytz\n",
    "import csv\n",
    "import time\n",
    "import smtplib\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.application import MIMEApplication\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "def auto_authenticate_primary_gmail():\n",
    "    \"\"\"Authenticates with primary Gmail account (token.json)\"\"\"\n",
    "    print(\"\\n=== Authenticating Primary Gmail Account ===\")\n",
    "    creds = None\n",
    "    token_path = 'token.json'\n",
    "    SCOPES = ['https://www.googleapis.com/auth/gmail.readonly']\n",
    "    \n",
    "    if os.path.exists(token_path):\n",
    "        print(\"Found primary token file, loading credentials...\")\n",
    "        try:\n",
    "            creds = Credentials.from_authorized_user_file(token_path, SCOPES)\n",
    "            if creds.expired and creds.refresh_token:\n",
    "                print(\"Primary credentials expired, refreshing...\")\n",
    "                creds.refresh(Request())\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading primary credentials: {e}\")\n",
    "            creds = None\n",
    "    \n",
    "    if not creds or not creds.valid:\n",
    "        print(\"No valid primary credentials found, initiating OAuth flow...\")\n",
    "        try:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file('client.json', SCOPES)\n",
    "            creds = flow.run_local_server(port=8080)\n",
    "            token_data = json.loads(creds.to_json())\n",
    "            token_data['creation_time'] = datetime.now(pytz.UTC).isoformat()\n",
    "            with open(token_path, 'w') as token:\n",
    "                json.dump(token_data, token)\n",
    "            print(\"Primary authentication successful! Token saved.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Primary authentication failed: {e}\")\n",
    "            raise\n",
    "    \n",
    "    try:\n",
    "        print(\"Building primary Gmail service...\")\n",
    "        gmail_service = build('gmail', 'v1', credentials=creds)\n",
    "        print(\"Primary Gmail service ready!\")\n",
    "        return gmail_service\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to build primary Gmail service: {e}\")\n",
    "        raise\n",
    "\n",
    "def auto_authenticate_secondary_gmail():\n",
    "    \"\"\"Authenticates with secondary Gmail account (token1.json)\"\"\"\n",
    "    print(\"\\n=== Authenticating Secondary Gmail Account ===\")\n",
    "    creds = None\n",
    "    token_path = 'token1.json'\n",
    "    SCOPES = ['https://www.googleapis.com/auth/gmail.readonly']\n",
    "    \n",
    "    if os.path.exists(token_path):\n",
    "        print(\"Found secondary token file, loading credentials...\")\n",
    "        try:\n",
    "            creds = Credentials.from_authorized_user_file(token_path, SCOPES)\n",
    "            if creds.expired and creds.refresh_token:\n",
    "                print(\"Secondary credentials expired, refreshing...\")\n",
    "                creds.refresh(Request())\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading secondary credentials: {e}\")\n",
    "            creds = None\n",
    "    \n",
    "    if not creds or not creds.valid:\n",
    "        print(\"No valid secondary credentials found, initiating OAuth flow...\")\n",
    "        try:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file('client1.json', SCOPES)\n",
    "            creds = flow.run_local_server(port=8081)\n",
    "            token_data = json.loads(creds.to_json())\n",
    "            token_data['creation_time'] = datetime.now(pytz.UTC).isoformat()\n",
    "            with open(token_path, 'w') as token:\n",
    "                json.dump(token_data, token)\n",
    "            print(\"Secondary authentication successful! Token saved.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Secondary authentication failed: {e}\")\n",
    "            raise\n",
    "    \n",
    "    try:\n",
    "        print(\"Building secondary Gmail service...\")\n",
    "        gmail_service = build('gmail', 'v1', credentials=creds)\n",
    "        print(\"Secondary Gmail service ready!\")\n",
    "        return gmail_service\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to build secondary Gmail service: {e}\")\n",
    "        raise\n",
    "\n",
    "def get_recent_emails(service, max_results=30):\n",
    "    \"\"\"Fetches the most recent emails from the inbox\"\"\"\n",
    "    print(f\"\\n=== Fetching {max_results} most recent emails ===\")\n",
    "    try:\n",
    "        results = service.users().messages().list(\n",
    "            userId=\"me\",\n",
    "            labelIds=['INBOX'],\n",
    "            maxResults=max_results\n",
    "        ).execute()\n",
    "        messages = results.get('messages', [])\n",
    "        print(f\"Found {len(messages)} emails in inbox\")\n",
    "        return messages\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching emails: {e}\")\n",
    "        return []\n",
    "\n",
    "def decode_base64(data):\n",
    "    \"\"\"Decodes base64 email content with proper padding\"\"\"\n",
    "    missing_padding = len(data) % 4\n",
    "    if missing_padding:\n",
    "        data += '=' * (4 - missing_padding)\n",
    "    return urlsafe_b64decode(data)\n",
    "\n",
    "def extract_email_body(service, message_id):\n",
    "    \"\"\"Extracts and decodes the email body content\"\"\"\n",
    "    try:\n",
    "        msg = service.users().messages().get(\n",
    "            userId=\"me\",\n",
    "            id=message_id,\n",
    "            format='full'\n",
    "        ).execute()\n",
    "        \n",
    "        payload = msg.get('payload', {})\n",
    "        \n",
    "        # Extract body from payload\n",
    "        body = \"\"\n",
    "        if 'body' in payload and 'data' in payload['body']:\n",
    "            body = decode_base64(payload['body']['data']).decode('utf-8', errors='ignore')\n",
    "        elif 'parts' in payload:\n",
    "            for part in payload['parts']:\n",
    "                if part.get('mimeType') in ['text/plain', 'text/html']:\n",
    "                    if 'body' in part and 'data' in part['body']:\n",
    "                        body = decode_base64(part['body']['data']).decode('utf-8', errors='ignore')\n",
    "                        break\n",
    "                elif 'parts' in part:\n",
    "                    for subpart in part['parts']:\n",
    "                        if subpart.get('mimeType') in ['text/plain', 'text/html']:\n",
    "                            if 'body' in subpart and 'data' in subpart['body']:\n",
    "                                body = decode_base64(subpart['body']['data']).decode('utf-8', errors='ignore')\n",
    "                                break\n",
    "        \n",
    "        return body if body else None\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing email {message_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_job_details(body):\n",
    "    \"\"\"Enhanced version specifically targeting Job ID fields in emails\"\"\"\n",
    "    job_data = {\n",
    "        'Title': None,\n",
    "        'Job_ID': None,\n",
    "        'Due_date': None\n",
    "    }\n",
    "    \n",
    "    if not body:\n",
    "        return job_data\n",
    "    \n",
    "    # Title extraction pattern (unchanged)\n",
    "    title_pattern = r'(?i)((?:Hybrid|Onsite|Remote)(?:\\s*/\\s*Local)?[^(]*\\(.*?\\)|(?:Hybrid|Onsite|Remote)(?:\\s*/\\s*Local)?.*?)(?=\\s+with\\s+|\\s*\\(|$|\\n)'\n",
    "    title_match = re.search(title_pattern, body)\n",
    "    if title_match:\n",
    "        job_data['Title'] = title_match.group(1).strip()\n",
    "    \n",
    "    # NEW: Focused Job ID extraction - looks for specific patterns indicating a Job ID field\n",
    "    job_id_patterns = [\n",
    "        r'(?i)(?:job\\s*|req\\s*|position\\s*)?(?:id\\s*|#\\s*|num\\s*|number\\s*)[:=\\s]*([A-Za-z]{2,}-?\\d{3,})',  # \"Job ID: VA-123456\"\n",
    "        r'(?i)(?:job\\s*|req\\s*|position\\s*)?(?:id\\s*|#\\s*|num\\s*|number\\s*)[:=\\s]*([A-Za-z]{2,}\\s*\\d{3,})',  # \"Job ID VA 123456\"\n",
    "        r'(?i)\\b(?:job\\s*|req\\s*|position\\s*)?(?:id\\s*|#\\s*|num\\s*|number\\s*)\\b\\s*[:-]?\\s*([A-Za-z]{2,}-?\\d{3,})',  # \"Job-ID:VA-123456\"\n",
    "        r'(?<!\\w)([A-Za-z]{2,}-?\\d{3,})(?!\\w)',  # Standalone ID as last resort\n",
    "    ]\n",
    "    \n",
    "    for pattern in job_id_patterns:\n",
    "        id_match = re.search(pattern, body)\n",
    "        if id_match:\n",
    "            job_id = id_match.group(1).strip()\n",
    "            # Clean up the job ID (remove spaces, normalize format)\n",
    "            job_id = re.sub(r'\\s+', '', job_id)  # Remove any spaces\n",
    "            job_id = job_id.upper()  # Convert to uppercase for consistency\n",
    "            job_data['Job_ID'] = job_id\n",
    "            break\n",
    "    \n",
    "    # Date extraction only if we found a Job ID\n",
    "    if job_data['Job_ID']:\n",
    "        # Look for dates in parentheses after Job ID\n",
    "        date_match = re.search(\n",
    "            r'{}\\s*\\((\\d+)\\)'.format(re.escape(job_data['Job_ID'])),\n",
    "            body\n",
    "        )\n",
    "        if date_match:\n",
    "            full_number = date_match.group(1)\n",
    "            last_four = full_number[-4:]\n",
    "            if len(last_four) == 4:\n",
    "                job_data['Due_date'] = f\"{last_four[:2]}/{last_four[2:]}\"\n",
    "    \n",
    "    return job_data\n",
    "\n",
    "def count_emails_for_job_id(service, job_id):\n",
    "    \"\"\"Count emails containing the job ID in the inbox.\"\"\"\n",
    "    try:\n",
    "        query = f'\"{job_id}\"'  # Using quotes for exact matching\n",
    "        results = service.users().messages().list(\n",
    "            userId=\"me\",\n",
    "            q=query,\n",
    "            labelIds=['INBOX']\n",
    "        ).execute()\n",
    "        return results.get('resultSizeEstimate', 0)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error counting emails for {job_id}: {e}\")\n",
    "        return 0\n",
    "\n",
    "def process_job_ids(csv_path, service):\n",
    "    \"\"\"Process job IDs from CSV and update with email counts.\"\"\"\n",
    "    try:\n",
    "        with open(csv_path, 'r') as file:\n",
    "            reader = csv.DictReader(file)\n",
    "            rows = list(reader)\n",
    "            fieldnames = reader.fieldnames\n",
    "            \n",
    "        if 'No_of_emails' not in fieldnames:\n",
    "            fieldnames.append('No_of_emails')\n",
    "            for row in rows:\n",
    "                row['No_of_emails'] = '0'\n",
    "    \n",
    "        for row in rows:\n",
    "            job_id = row.get('Job ID', '').strip() or row.get('Job_ID', '').strip()\n",
    "            if job_id:\n",
    "                logging.info(f\"Searching for emails with job ID: {job_id}\")\n",
    "                email_count = count_emails_for_job_id(service, job_id)\n",
    "                row['No_of_emails'] = str(email_count)\n",
    "                logging.info(f\"Found {email_count} emails for {job_id}\")\n",
    "                time.sleep(0.5)  # Rate limiting\n",
    "    \n",
    "        with open(csv_path, 'w', newline='') as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(rows)\n",
    "    \n",
    "        logging.info(f\"Successfully updated CSV file: {csv_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing CSV file: {e}\")\n",
    "        raise\n",
    "\n",
    "def send_results_email(excel_path, recipient_email):\n",
    "    \"\"\"Send the results Excel file as an email attachment\"\"\"\n",
    "    try:\n",
    "        # Email configuration - REPLACE THESE WITH YOUR DETAILS\n",
    "        sender_email = \"gorintalakavya@gmail.com\"\n",
    "        password = \"tihr qpwm pwwv dtzf\"  # Use App Password if 2FA is enabled\n",
    "        smtp_server = \"smtp.gmail.com\"\n",
    "        smtp_port = 587\n",
    "        \n",
    "        # Create message container\n",
    "        msg = MIMEMultipart()\n",
    "        msg['From'] = sender_email\n",
    "        msg['To'] = recipient_email\n",
    "        msg['Subject'] = \"Job Application Tracker Results\"\n",
    "        \n",
    "        # Email body\n",
    "        body = f\"\"\"Please find attached the latest job application tracking results.\n",
    "        \n",
    "        Report generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "        \n",
    "        This file contains:\n",
    "        - Job Titles\n",
    "        - Job IDs\n",
    "        - Due Dates\n",
    "        - Email Counts\n",
    "        \"\"\"\n",
    "        msg.attach(MIMEText(body, 'plain'))\n",
    "        \n",
    "        # Attach Excel file\n",
    "        with open(excel_path, 'rb') as file:\n",
    "            part = MIMEApplication(file.read(), Name=\"Job_Tracker_Report.xlsx\")\n",
    "        part['Content-Disposition'] = f'attachment; filename=\"{os.path.basename(excel_path)}\"'\n",
    "        msg.attach(part)\n",
    "        \n",
    "        # Send email\n",
    "        with smtplib.SMTP(smtp_server, smtp_port) as server:\n",
    "            server.starttls()\n",
    "            server.login(sender_email, password)\n",
    "            server.send_message(msg)\n",
    "        \n",
    "        print(f\"\\nResults successfully emailed to {recipient_email}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nFailed to send email: {str(e)}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to process both Gmail accounts\"\"\"\n",
    "    try:\n",
    "        # Initialize results table\n",
    "        results = []\n",
    "        \n",
    "        # Step 1: Authenticate with primary Gmail (extraction account)\n",
    "        primary_service = auto_authenticate_primary_gmail()\n",
    "        \n",
    "        # Step 2: Get recent emails from primary account\n",
    "        messages = get_recent_emails(primary_service, max_results=25)\n",
    "        \n",
    "        if not messages:\n",
    "            print(\"No emails found in primary inbox\")\n",
    "            return\n",
    "        \n",
    "        # Step 3: Process each email from primary account\n",
    "        print(\"\\n=== Processing Emails from Primary Account ===\")\n",
    "        for i, message in enumerate(messages, 1):\n",
    "            message_id = message['id']\n",
    "            print(f\"\\nProcessing Email #{i} (ID: {message_id})\")\n",
    "            \n",
    "            body = extract_email_body(primary_service, message_id)\n",
    "            \n",
    "            if body:\n",
    "                job_details = extract_job_details(body)\n",
    "                job_details['Email ID'] = message_id  # Temporarily keep for processing\n",
    "                results.append(job_details)\n",
    "                \n",
    "                print(f\"Title: {job_details['Title']}\")\n",
    "                print(f\"Job_ID: {job_details['Job_ID']}\")\n",
    "                print(f\"Due_date: {job_details['Due_date']}\")\n",
    "            else:\n",
    "                print(\"No body content could be extracted\")\n",
    "        \n",
    "        # Create DataFrame and filter rows where Title is missing\n",
    "        df = pd.DataFrame(results)\n",
    "        df = df.dropna(subset=['Title'])  # Remove rows with missing Title\n",
    "        \n",
    "        if not df.empty:\n",
    "            # Create temporary CSV for secondary account processing\n",
    "            temp_csv = 'temp_duedates.csv'\n",
    "            df = df.drop(columns=['Email ID'], errors='ignore')\n",
    "            df.to_csv(temp_csv, index=False)\n",
    "            \n",
    "            # Step 4: Authenticate with secondary Gmail (counting account)\n",
    "            secondary_service = auto_authenticate_secondary_gmail()\n",
    "            \n",
    "            # Step 5: Process CSV with secondary account to add No_of_emails\n",
    "            process_job_ids(temp_csv, secondary_service)\n",
    "            \n",
    "            # Load the final data\n",
    "            final_df = pd.read_csv(temp_csv)\n",
    "            \n",
    "            # Remove temporary CSV\n",
    "            os.remove(temp_csv)\n",
    "            \n",
    "            # Display final table\n",
    "            print(\"\\n\" + \"=\"*100)\n",
    "            print(\"FINAL RESULTS\".center(100))\n",
    "            print(\"=\"*100)\n",
    "            \n",
    "            # Configure display options\n",
    "            pd.set_option('display.max_colwidth', 40)\n",
    "            pd.set_option('display.width', 120)\n",
    "            pd.set_option('display.colheader_justify', 'center')\n",
    "            \n",
    "            # Create formatted table\n",
    "            table = final_df.to_markdown(\n",
    "                tablefmt=\"grid\",\n",
    "                stralign=\"left\",\n",
    "                numalign=\"left\",\n",
    "                index=False\n",
    "            )\n",
    "            \n",
    "            # Add left margin to each line\n",
    "            margined_table = [f\"    {line}\" for line in table.split('\\n')]\n",
    "            print('\\n'.join(margined_table))\n",
    "            print(\"=\"*100)\n",
    "            \n",
    "            # Save to Excel with formatting\n",
    "            excel_path = 'duedates_formatted.xlsx'\n",
    "            try:\n",
    "                import xlsxwriter\n",
    "                writer = pd.ExcelWriter(excel_path, engine='xlsxwriter')\n",
    "                final_df.to_excel(writer, index=False, sheet_name='Job Details')\n",
    "                \n",
    "                workbook = writer.book\n",
    "                worksheet = writer.sheets['Job Details']\n",
    "                \n",
    "                # Formatting\n",
    "                header_format = workbook.add_format({\n",
    "                    'bold': True,\n",
    "                    'text_wrap': True,\n",
    "                    'valign': 'top',\n",
    "                    'align': 'center',\n",
    "                    'border': 1\n",
    "                })\n",
    "                \n",
    "                cell_format = workbook.add_format({\n",
    "                    'text_wrap': True,\n",
    "                    'valign': 'top',\n",
    "                    'align': 'left',\n",
    "                    'border': 1\n",
    "                })\n",
    "                \n",
    "                # Apply formatting\n",
    "                for col_num, value in enumerate(final_df.columns.values):\n",
    "                    worksheet.write(0, col_num, value, header_format)\n",
    "                \n",
    "                for row in range(1, len(final_df)+1):\n",
    "                    for col in range(len(final_df.columns)):\n",
    "                        worksheet.write(row, col, str(final_df.iloc[row-1, col]), cell_format)\n",
    "                \n",
    "                # Auto-adjust column widths\n",
    "                for i, col in enumerate(final_df.columns):\n",
    "                    max_len = max((\n",
    "                        final_df[col].astype(str).map(len).max(),\n",
    "                        len(col)\n",
    "                    )) + 2\n",
    "                    worksheet.set_column(i, i, max_len)\n",
    "                \n",
    "                writer.close()\n",
    "                print(f\"\\nFinal results saved to '{excel_path}'\")\n",
    "                  \n",
    "                # Send email with results - REPLACE WITH ACTUAL RECIPIENT\n",
    "                send_results_email(excel_path, \"nandini.ka12345@gmail.com\")\n",
    "                \n",
    "            except ImportError:\n",
    "                print(\"\\nError: xlsxwriter not installed - cannot create Excel file\")\n",
    "                print(\"Install with: pip install xlsxwriter\")\n",
    "                # Fallback to CSV if Excel fails\n",
    "                csv_path = 'duedates.csv'\n",
    "                final_df.to_csv(csv_path, index=False)\n",
    "                print(f\"Results saved to '{csv_path}'\")\n",
    "            \n",
    "        else:\n",
    "            print(\"\\nNo valid job details found (all rows filtered out)\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4438b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rsp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
